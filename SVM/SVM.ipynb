{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb167f0e",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "#### Ruixuan Dong\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    " - [The Support Vector Machine classification algorithm](#1)\n",
    "    - [Optimal separating hyperplane](#11)\n",
    "    - [Algorithm Explanation](#12)\n",
    " - [Research Problem -- Using synthetic data to classify different systolic levels](#2)\n",
    "    - [Overview of the Problem set](#21)\n",
    "    - [Implement SVM on Synthetic data](#22)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf428c4",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "### 1 - The Support Vector Machine classification algorithm\n",
    "\n",
    "First, let's run the cell below to import all the packages that we will need during this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92359f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec42fda",
   "metadata": {},
   "source": [
    "<a name='11'></a>\n",
    "### 1-1 Optimal separating hyperplane\n",
    "\n",
    "\n",
    "A refinement of the perceptron learning algorithm is the optimal separating hyperplane. The goal is to find a hyperplane which separates the two classes and maximizes the distance from the hyperplane to the closest point from either class. In so doing, this leads to a unique solution to separating hyperplane problem, and tends to lead to better prediction accuracy on new data points.\n",
    "\n",
    "The optimization problem for finding an optimal separating hyperplane is\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta, \\beta_0, ||\\beta||_2=1} M \\quad \\text{subject to} \\quad y_i(x_i'\\beta + \\beta_0) \\geq M, \\quad 1 \\leq i \\leq n. \\quad \\quad (1)\n",
    "\\end{align*}\n",
    "\n",
    "Notice, the requirement that $y_i(x_i'\\beta + \\beta_0) \\geq M$ will ensure that all points are correctly classified and the distance from each point to the separating hyperplane is at least $M$. We can rewrite this in a more familiar way by removing the unit length constraint on $\\beta$ by letting $\\alpha = \\beta ||\\beta||_2$ and $\\alpha_0 = \\beta_0 ||\\beta||_2$, so that the constraint can be written as $y_i(x_i'\\alpha+\\alpha_0)/||\\alpha||_2 \\geq M$, $1 \\leq i \\leq n$. Hence, we see that an equivalent formulation is\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta, \\beta_0} M \\quad \\text{subject to} \\quad y_i(x_i'\\beta + \\beta_0) \\geq M||\\beta||_2, \\quad 1 \\leq i \\leq n.\n",
    "\\end{align*}\n",
    "\n",
    "For any $\\beta, \\beta_0$ satisfying $y_i(x_i'\\beta + \\beta_0) \\geq M||\\beta||_2$, any scalar multiple of $\\beta, \\beta_0$ also satisfies the constraint. Thus, we can simply set $M = \\frac{1}{||\\beta||_2}$, so that we may (finally) write the problem as\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\beta, \\beta_0} \\frac{1}{2}||\\beta||_2^2 \\quad \\text{subject to} \\quad y_i(x_i'\\beta + \\beta_0) \\geq 1, \\quad 1 \\leq i \\leq n. \\quad \\quad (2)\n",
    "\\end{align*}\n",
    "\n",
    "From this formulation, we can see that the optimal separating hyperplane is that which falls between two parallel hyperplanes between which no points live (although these hyperplanes may pass through points). The distance from these two hyperplanes and the separating hyperplane is $\\frac{1}{||\\beta||_2}$ and our objective is to maximize this distance. This is illustrated in Figure 1 below. These hyperplanes are shown in blue, with the orange hyperplane being the optimal separating hyperplane. In Figure 5, this is the distance from the circled points to the orange hyperplane.\n",
    "\n",
    "Note that the dual problem (an equivalent formulation) to the above is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_\\gamma \\left( \\min_{\\beta, \\beta_0} \\left( \\frac{1}{2}||\\beta||_2^2 + \\sum_{i=1}^{n} \\gamma_i [y_i(x_i'\\beta + \\beta_0) - 1] \\right) \\right) \\quad \\text{subject to} \\quad \\gamma_i \\geq 0.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where the $\\gamma_i$ are the dual variables (i.e., lagrangian multipliers).\n",
    "\n",
    "<img src=\"hyperplane.png\" alt=\"Figure 1\" width=\"400\" height=\"300\">\n",
    "<p style=\"text-align:left;\">Figure 1: Plot showing data which can be separated by a hyperplane, but linear regression\n",
    "on $y$ (solid black) yields a decision boundary which does not separate the data. The optimal separating hyperplane is given in orange, with the support points circled in blue.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e431b",
   "metadata": {},
   "source": [
    "<a name='12'></a>\n",
    "### 1-2 Algorithm Explanation\n",
    "\n",
    "Support vector machines can be thought of as a generalization of the optimal separating hyperplane. For the proceeding discussion, let us define the space between the two hyperplanes which are determined by the support points as the margin. Based on the original formulation of the optimal separating hyperplane, we can think of the margin as having a width equal to $2\\hat{M}$, where $\\hat{M}$ is the solution to (1).\n",
    "\n",
    "Suppose now that the data are not separable by a hyperplane. Rather than requiring \n",
    "\n",
    "\\begin{align*}\n",
    "y_i(x_i'\\beta + \\beta_0) \\geq M,\n",
    "\\end{align*}\n",
    "\n",
    "as in the optimal separating hyperplane, we will relax this restriction to require \n",
    "\n",
    "\\begin{align*}\n",
    "y_i(x_i'\\beta + \\beta_0) \\geq M(1 - \\xi_i) = M - M\\xi_i.\n",
    "\\end{align*}\n",
    "\n",
    "where $\\xi_1, \\ldots, \\xi_n$ are slack variables such that\n",
    "$\\xi_i \\geq 0$,\n",
    "$\\sum_{i=1}^{n} \\xi_i \\leq \\text{constant}$.\n",
    "\n",
    "Intuitively, this is straightforward to interpret – the value of $\\xi_i$ is the proportional amount by which the prediction $x_i'\\beta + \\beta_0$ is on the incorrect side of the margin (relative to the hyperplanes which determine the margin). Misclassifications occur when $\\xi_i > 1$. Thus, a bound on $\\sum_{i=1}^{n} \\xi_i$ controls the total distance from all misclassified points to the correct side of the margin. Bounding this sum at $L$ bounds the number of training misclassifications at $L$. This is illustrated in Figure 2.\n",
    "\n",
    "Following the reformulation of (2) into (3), we can write the corresponding optimization problem as\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\beta, \\beta_0} \\frac{1}{2}||\\beta||_2^2 \\quad \\text{subject to} \\quad \n",
    "\\begin{cases}\n",
    "    y_i(x_i'\\beta + \\beta_0) \\geq 1 - \\xi_i & \\text{for } 1 \\leq i \\leq n \\\\\n",
    "    \\xi_i \\geq 0 & \\text{for } 1 \\leq i \\leq n \\\\\n",
    "    \\sum_{i=1}^{n} \\xi_i \\leq \\text{constant}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "This optimization is often presented in its dual form\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\gamma, \\mu} \\left( \\min_{\\beta, \\beta_0, \\xi} \\left( \\frac{1}{2}||\\beta||_2^2 + L \\sum_{i=1}^{n} \\xi_i - \\sum_{i=1}^{n} \\gamma_i [y_i(x_i'\\beta + \\beta_0) - (1 - \\xi_i)] - \\sum_{i=1}^{n} \\mu_i \\xi_i \\right) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "subject to $\\mu_i \\geq 0$, $\\xi_i \\geq 0$, $\\gamma_i \\geq 0$ for $1 \\leq i \\leq n$.\n",
    "\n",
    "where $L$ is some constant corresponding to the constraint on $\\sum_{i=1}^{n} \\xi_i$\n",
    "from before.\n",
    "\n",
    "<img src=\"SVM.png\" alt=\"Figure 2\" width=\"600\" height=\"300\">\n",
    "<p style=\"text-align:left;\">Figure 2: A visualization of the slack variables in support vector machines. The points labeled $\\xi^*_j$ are on the wrong side of their margin by an amount $\\xi^*_j = M\\xi_j$. Points on the correct side have $\\xi^*_j = 0$. The margin is maximized subject to a total budget of $\\sum_{j=1}^{n} \\xi_j \\leq$ a constant.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776985b6",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "### 2 - Using synthetic data to classify different systolic levels\n",
    "\n",
    "In this part, we use the dataset generated by Synthetic SCG data generator https://github.com/wsonguga/DataDemo. In the data file, each row includes sensor data (10 seconds * 100Hz) + HeartRate + RespiratoryRate + SystolicBloodPressure + DiastolicBloodPressure.\n",
    "\n",
    "Now we would like to give a introduction about the synthetic SCG dataset we generated. \n",
    "\n",
    "Based on Synthetic SCG data generator, we generated an artificial (synthetic) scg signal of a given duration (10 seconds, i.e. duration=10) and sampling rate (100Hz, i.e. sampling rate=100) using a model based on Daubechies wavelets to roughly approximate cardiac cycles.\n",
    "\n",
    "Besides, we set \n",
    " - heart rate to be randomly chosen from the intgers range from 50 to 150, with the desired heart rate standard deviation (beats per minute) equal to 1.\n",
    " - respiratory rate to be randomly chosen from the intgers range from 10 to 30\n",
    " - diastolic blood pressure to be randomly chosen from the intgers range from 60 to 99\n",
    " - systolic blood pressure to be randomly chosen from the intgers range from 100 to 160\n",
    "\n",
    "The sample size of the current dataset is 6,000 in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08560b49",
   "metadata": {},
   "source": [
    "<a name='21'></a>\n",
    "### 2-1 Overview of the Problem set \n",
    "**Problem Statement:** The generated dataset containing: \n",
    "- a dataset set (\"total_large.csv\") of 6,000 samples labeled as lower (100<=systolic blood pressure<140) or higher (140<=systolic blood pressure<=160) \n",
    "- each sample is of shape (1, 1003) where 1003 is for the 1000-d signal and heart rate, respiratory rate and diastolic blood pressure\n",
    "\n",
    "In this part, we will build a simple SVM classifier that can correctly classify samples as lower or higher (SBP).\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d09f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>1000</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.993225e-08</td>\n",
       "      <td>3.517546e-07</td>\n",
       "      <td>3.597235e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-2.597618e-07</td>\n",
       "      <td>1.085503e-07</td>\n",
       "      <td>-2.178073e-07</td>\n",
       "      <td>-6.006744e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.069616e-08</td>\n",
       "      <td>-2.657577e-08</td>\n",
       "      <td>-2.138259e-08</td>\n",
       "      <td>-1.922973e-08</td>\n",
       "      <td>-2.095749e-08</td>\n",
       "      <td>-2.288238e-08</td>\n",
       "      <td>124.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.925762e-08</td>\n",
       "      <td>5.413484e-07</td>\n",
       "      <td>4.186926e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>2.296199e-06</td>\n",
       "      <td>-1.955847e-06</td>\n",
       "      <td>1.334962e-06</td>\n",
       "      <td>-3.836647e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.732173e-08</td>\n",
       "      <td>-2.730453e-08</td>\n",
       "      <td>-2.924664e-08</td>\n",
       "      <td>-2.703059e-08</td>\n",
       "      <td>-2.099285e-08</td>\n",
       "      <td>-1.787339e-08</td>\n",
       "      <td>124.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.884543e-08</td>\n",
       "      <td>6.735418e-08</td>\n",
       "      <td>9.212940e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>2.197260e-06</td>\n",
       "      <td>-1.165844e-06</td>\n",
       "      <td>8.744602e-07</td>\n",
       "      <td>-6.691456e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.810176e-08</td>\n",
       "      <td>-2.713190e-08</td>\n",
       "      <td>-2.622019e-08</td>\n",
       "      <td>-2.368465e-08</td>\n",
       "      <td>-2.027036e-08</td>\n",
       "      <td>-1.836652e-08</td>\n",
       "      <td>121.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1             2             3         4         5         6  \\\n",
       "0  3.993225e-08  3.517546e-07  3.597235e-06  0.000006 -0.000003  0.000001   \n",
       "1  3.925762e-08  5.413484e-07  4.186926e-06  0.000007 -0.000002 -0.000002   \n",
       "2  2.884543e-08  6.735418e-08  9.212940e-07  0.000005  0.000004 -0.000004   \n",
       "\n",
       "              7             8             9            10  ...           995  \\\n",
       "0 -2.597618e-07  1.085503e-07 -2.178073e-07 -6.006744e-08  ... -3.069616e-08   \n",
       "1  2.296199e-06 -1.955847e-06  1.334962e-06 -3.836647e-07  ... -2.732173e-08   \n",
       "2  2.197260e-06 -1.165844e-06  8.744602e-07 -6.691456e-07  ... -2.810176e-08   \n",
       "\n",
       "            996           997           998           999          1000  \\\n",
       "0 -2.657577e-08 -2.138259e-08 -1.922973e-08 -2.095749e-08 -2.288238e-08   \n",
       "1 -2.730453e-08 -2.924664e-08 -2.703059e-08 -2.099285e-08 -1.787339e-08   \n",
       "2 -2.713190e-08 -2.622019e-08 -2.368465e-08 -2.027036e-08 -1.836652e-08   \n",
       "\n",
       "   heart_rate  respiratory_rate  systolic  diastolic  \n",
       "0       124.0              17.0     103.0       68.0  \n",
       "1       124.0              30.0     102.0       91.0  \n",
       "2       121.0              10.0     156.0       92.0  \n",
       "\n",
       "[3 rows x 1004 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [str(i) for i in range(1, 1001)] + ['heart_rate', 'respiratory_rate', 'systolic', 'diastolic']\n",
    "total = pd.read_csv('total.csv', \n",
    "                 header=None, \n",
    "                 names=column_names)\n",
    "total.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0c0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal2matrix(total):\n",
    "    total = total.values\n",
    "\n",
    "    numberOfLines = len(total)\n",
    "    returnMat = np.zeros((numberOfLines, 1003))\n",
    "    classLabelVector = []\n",
    "    index = 0\n",
    "\n",
    "    for line in total:\n",
    "        returnMat[index, :1002] = line[:1002]\n",
    "        returnMat[index, 1002] = line[1003]\n",
    "        if 100 <=line[1002]< 140:\n",
    "            classLabelVector.append(1)\n",
    "        elif 140 <=line[1002]<= 160:\n",
    "            classLabelVector.append(2)\n",
    "        index += 1\n",
    "    return returnMat, classLabelVector\n",
    "\n",
    "def autoNorm(dataSet):\n",
    "    minVals = dataSet.min(0)\n",
    "    maxVals = dataSet.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normDataSet = np.zeros(np.shape(dataSet))\n",
    "    m = dataSet.shape[0]\n",
    "    normDataSet = dataSet - np.tile(minVals, (m, 1))\n",
    "    normDataSet = normDataSet / np.tile(ranges, (m, 1))\n",
    "    return normDataSet, ranges, minVals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e77ba1",
   "metadata": {},
   "source": [
    "Overall, these functions are used for preprocessing data. `signal2matrix` converts data into a matrix format with class labels, and `autoNorm` performs feature scaling to normalize the dataset. These preprocessing steps are often important before applying machine learning algorithms to data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba2be2",
   "metadata": {},
   "source": [
    "<a name='22'></a>\n",
    "### 2-2 Implement SVM on Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b05c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.33%\n"
     ]
    }
   ],
   "source": [
    "X, y = signal2matrix(total)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "clf = svm.SVC(kernel='linear')  # You can choose different kernels (linear, rbf, etc.)\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa2bdab",
   "metadata": {},
   "source": [
    "This code demonstrates the process of preparing data, splitting it into training and testing sets, training an SVM classifier, making predictions, and evaluating the classifier's performance using accuracy. The reported accuracy indicates how well the SVM classifier is able to classify the test data based on the provided features. In a binary classification context(our e), this means that about 64.33% of the test data points were assigned the correct class label by the classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
