{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bbb48f",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "Ruixuan Dong\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    " - [Introduction](#0)\n",
    " - [Model averaging](#1)\n",
    "     - [Stacking](#11)\n",
    "     - [Super learning](#12)\n",
    " - [Simulated Dataset](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c3fe2",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this section, we discuss ensemble methods. These are, loosely speaking, approaches for combining many learners to create a more powerful committee. Similarly, we can think of boosting as an ensemble method as well. In this section, we will focus on two approaches for ensemble learning: stacking and the so-called superlearner. Arguably, these are both just special cases of model averaging.\n",
    "\n",
    "While some ideas in this section may seem simple, these approaches are extremely powerful and often used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da355d",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "\n",
    "## 1 Model averaging\n",
    "\n",
    "To begin our discussion of model averaging, let us recall the bagging (bootstrap aggregated) estimator we discussed earlier in the course. To fit the bagging estimator, we constructed B independent bootstrapped datasets from our full training data \\\\({(y_i, x_i)}_{i=1}^n\\\\). Then, we fit a prediction model to each of the bootstrapped datasets. Let \\\\(\\hat{f}_b(x)\\\\) denote the prediction based on the model from the bth bootstrapped dataset. The bagging estimator was then defined as simply the average of B different prediction models \\\\(\\hat{f}_b\\\\), that is:\n",
    "\n",
    "\\\\[ \\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{f}_b(x) \\\\]\n",
    "\n",
    "The utility of bagging, we argued, was that it could reduce the variance by virtue of averaging.\n",
    "\n",
    "We can alternatively think of \\\\(\\hat{f}_{\\text{bag}}\\\\) as a model averaging approach: after all, it is defined as simply the average of $B$ different prediction models (if we ignore that each is fit to a different bootstrapped dataset). From this perspective, we can view model aggregation as a slightly different problem: given models \\\\(\\{f_1, ..., f_B\\}\\\\), how can we combine them in order to achieve the best combination (in the sense that it achieves the best prediction accuracy)? Taking their average is one approach, but perhaps a weighted average may perform better. Stacking is one approach to achieve an aggregated model based on a weighted average.\n",
    "\n",
    "Let \\\\(\\hat{f}_1(x), \\ldots, \\hat{f}_M(x)\\\\) be predictions coming from M distinct fitted models. Given these predictions, under squared-error loss, we seek weights \\\\(w = (w_1, \\ldots, w_M)\\\\) such that:\n",
    "\n",
    "\\\\[ \\hat{w} =  \\underset{w}{\\text{arg min}}\\ \\mathbb{E}\\left[\\left(Y - \\sum_{m=1}^M w_m \\hat{f}_m(x)\\right)^2\\right] \\\\]\n",
    "\n",
    "where the expectation is taken with respect to the distribution of $Y$ given the predictors. With \\\\(\\hat{F}(x) \\equiv [\\hat{f}_1(x), \\ldots, \\hat{f}_M(x)]\\\\), it is straightforward to verify that:\n",
    "\n",
    "\\\\[ \\hat{w} = \\mathbb{E}\\left[\\hat{F}(x)\\hat{F}(x)'\\right]^{-1} \\mathbb{E}\\left[\\hat{F}(x)Y\\right]. \\\\]\n",
    "\n",
    "Given this \\\\(\\hat{w}\\\\), one can show that:\n",
    "\n",
    "\\\\[ \\mathbb{E}\\left[\\left(Y - \\sum_{m=1}^M \\hat{w}_m \\hat{f}_m(x)\\right)^2\\right] \\leq \\mathbb{E}\\left[\\left(Y - \\hat{f}_m(x)\\right)^2\\right], \\, \\forall m, \\\\]\n",
    "\n",
    "meaning that at the population level, the weighted average model \\\\(\\sum_{m=1}^M \\hat{w}_m \\hat{f}_m(x)\\\\) is never worse than any individual model \\\\(\\hat{f}_m\\\\) in terms of squared error.\n",
    "\n",
    "Of course, while this is nice in theory, such an estimator cannot be put to use in practice. Replacing the expectations above with the sample counterparts often leads to poor prediction. For example, consider the case that \\\\(\\hat{f}_m(x)\\\\) denotes the best subset regression model of size m. Then, using the sample version of this approach would also put all weight on \\\\(\\hat{f}_M(x)\\\\), the largest best subsets model (i.e., \\\\(\\hat{w}_M = 1\\\\) and \\\\(\\hat{w}_m = 0\\\\) for all $m \\neq M$). This is intuitive since this approach only accounts for model fit, not model complexity (as \\\\(\\hat{f}_M\\\\) is the most complex model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3ceb3",
   "metadata": {},
   "source": [
    "<a name='11'></a>\n",
    "\n",
    "### 1.1 Stacking\n",
    "One approach for model combination which avoids this issue is stacking or stacked generalizations. Let \\\\(\\hat{f}^{-i}_m(x)\\\\) be the prediction at $x$ using model $m$, applied to a dataset with the $i$th sample removed. The stacked estimate of the weights $w$ is then defined as:\n",
    "\n",
    "\\\\[ \\hat{w}_{\\text{st}} = \\underset{w}{\\text{arg min}} \\sum_{i=1}^{n} \\left( y_i - \\sum_{m=1}^{M} w_m f^{-i}_m(x_i) \\right)^2 \\\\]\n",
    "\n",
    "so that the stacked estimate at x is \\\\(\\sum_{m=1}^{M} \\hat{w}^{\\text{st}}_m \\hat{f}_m(x)\\\\) where \\\\(\\hat{f}_m\\\\) is the $m$th model fit to the entire dataset. This approach can be improved by imposing a non-negativity and sum-to-one constraint on the $w$\n",
    "\n",
    "\\\\[ \\tilde{w}_{\\text{st}} = \\underset{w}{\\text{argmin}} \\left( \\sum_{i=1}^{n} \\left( y_i - \\sum_{m=1}^{M} w_m f_{m}^{-i}(x_i) \\right)^2 \\right), \\ \\sum_{m=1}^{M} w_m = 1, w_m \\geq 0 \\, \\forall m. \\\\]\n",
    "\n",
    "By using cross-validation preidctions $\\hat{f}_m^{-i}(x_i)$, stacking avoids giving unfairly high weight to models with higher complexity.\n",
    "\n",
    "This approach is more general than described here. For example, we can replace squared error with an alternative loss; or could modify the criterion so that the weight estimation criterion also depends on x, the point at which a new prediction is ultimately desired. One could, in principle, assign more weight to models where points near $x$ predict well.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a7b32",
   "metadata": {},
   "source": [
    "<a name='12'></a>\n",
    "\n",
    "### 1.2 Super learning\n",
    "\n",
    "One approach for model combination, which is a popular variation of generalized stacking, is super learning. The prediction model output at the end of the procedure is the so-called super learner. This section will follow closely from Van der Laan et al. (2007). This method has achieved widespread use due to some nice theoretical results and its well-documented, easy-to-use software.\n",
    "\n",
    "To formalize our discussion, suppose we have observed n realizations of the random pair \\\\((Y_i, X_i)\\\\). The goal is to estimate \\\\(E(Y | X) \\equiv f^*(X)\\\\). For a particular dataset, suppose we have a library of estimators of \\\\(f^*\\\\). These estimators can be simple (e.g., a stump or a linear regression model) or quite complex (e.g., a random forest or boosted tree). The key is that each \\\\(\\hat{f}\\\\) is a function mapping the training data to a prediction.\n",
    "\n",
    "Let L denote the library of estimators and let K(n) denote the cardinality of L. The super learning algorithm proceeds as follows:\n",
    "\n",
    "- **1**: Fit each estimator in L to the entire training dataset to obtain predictions \\\\(\\hat{f}_1(x), \\ldots, \\hat{f}_{K(n)}(x)\\\\).\n",
    "- **2**: Split the training data into a training and validation sample according to a V-fold cross-validation scheme.\n",
    "- **3**: For each fold, fit each algorithm in L on the training data, and store the predictions on the corresponding validation data.\n",
    "- **4**: Stack the predictions from each estimator together to create an n × K(n) matrix of predictions.\n",
    "- **5**: Propose a family of weighted combinations of the candidate estimators indexed by a weight vector.\n",
    "- **6**: Determine the weight vector that minimizes the cross-validated risk of the candidate estimator.\n",
    "- **7**: Output the Super Learner, which is a weighted combination of the fitted models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a93127",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "### 2 Simulated Dataset\n",
    "\n",
    "In this part, we use the dataset generated by Synthetic SCG data generator https://github.com/wsonguga/DataDemo. In the data file, each row includes sensor data (10 seconds * 100Hz) + HeartRate + RespiratoryRate + SystolicBloodPressure + DiastolicBloodPressure.\n",
    "\n",
    "Now we would like to give a introduction about the synthetic SCG dataset we generated. \n",
    "\n",
    "Based on Synthetic SCG data generator, we generated an artificial (synthetic) scg signal of a given duration (10 seconds, i.e. duration=10) and sampling rate (100Hz, i.e. sampling rate=100) using a model based on Daubechies wavelets to roughly approximate cardiac cycles.\n",
    "\n",
    "Besides, we set \n",
    " - heart rate to be randomly chosen from the intgers range from 50 to 150, with the desired heart rate standard deviation (beats per minute) equal to 1.\n",
    " - respiratory rate to be randomly chosen from the intgers range from 10 to 30\n",
    " - diastolic blood pressure to be randomly chosen from the intgers range from 60 to 99\n",
    " - systolic blood pressure to be randomly chosen from the intgers range from 100 to 160\n",
    "\n",
    "The sample size of the current dataset is 6,000 in total.\n",
    "\n",
    "\n",
    "**Problem Statement:** The generated dataset containing: \n",
    "- a dataset set (\"lower.csv\") of 3,000 samples labeled as lower (100<=systolic blood pressure<140) \n",
    "- a dataset set (\"higher.csv\") of 3,000 samples labeled as higher (140<=systolic blood pressure<=160) \n",
    "- each sample is of shape (1, 1003) where 1003 is for the 1000-d signal and heart rate, respiratory rate and diastolic blood pressure\n",
    "\n",
    "In this part, we will build a simple kNN classifier that can correctly classify samples as lower or higher (SBP).\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2487573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63544b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>1000</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.439234e-08</td>\n",
       "      <td>2.583753e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-4.897503e-07</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-2.029141e-07</td>\n",
       "      <td>3.029687e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.750468e-08</td>\n",
       "      <td>-3.504179e-08</td>\n",
       "      <td>-3.266654e-08</td>\n",
       "      <td>-2.969555e-08</td>\n",
       "      <td>-2.688206e-08</td>\n",
       "      <td>-2.599564e-08</td>\n",
       "      <td>109.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.781177e-08</td>\n",
       "      <td>3.850786e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-3.642447e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>8.308896e-07</td>\n",
       "      <td>-1.850758e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.937486e-08</td>\n",
       "      <td>-3.615418e-08</td>\n",
       "      <td>-3.250324e-08</td>\n",
       "      <td>-2.930146e-08</td>\n",
       "      <td>-2.813366e-08</td>\n",
       "      <td>-2.915194e-08</td>\n",
       "      <td>131.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.434446e-08</td>\n",
       "      <td>2.098668e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-1.939304e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-9.990558e-07</td>\n",
       "      <td>3.452373e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.199401e-08</td>\n",
       "      <td>-2.472291e-08</td>\n",
       "      <td>-1.890941e-08</td>\n",
       "      <td>-1.882332e-08</td>\n",
       "      <td>-2.188260e-08</td>\n",
       "      <td>-2.335538e-08</td>\n",
       "      <td>128.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1             2         3         4         5         6  \\\n",
       "0  5.439234e-08  2.583753e-07  0.000001  0.000004  0.000008  0.000006   \n",
       "1  5.781177e-08  3.850786e-07  0.000002  0.000007  0.000007 -0.000001   \n",
       "2  3.434446e-08  2.098668e-07  0.000003  0.000006 -0.000003  0.000002   \n",
       "\n",
       "              7         8             9            10  ...           995  \\\n",
       "0 -4.897503e-07 -0.000004 -2.029141e-07  3.029687e-06  ... -3.750468e-08   \n",
       "1 -3.642447e-06  0.000002  8.308896e-07 -1.850758e-06  ... -3.937486e-08   \n",
       "2 -1.939304e-06  0.000001 -9.990558e-07  3.452373e-07  ... -3.199401e-08   \n",
       "\n",
       "            996           997           998           999          1000  \\\n",
       "0 -3.504179e-08 -3.266654e-08 -2.969555e-08 -2.688206e-08 -2.599564e-08   \n",
       "1 -3.615418e-08 -3.250324e-08 -2.930146e-08 -2.813366e-08 -2.915194e-08   \n",
       "2 -2.472291e-08 -1.890941e-08 -1.882332e-08 -2.188260e-08 -2.335538e-08   \n",
       "\n",
       "   heart_rate  respiratory_rate  systolic  diastolic  \n",
       "0       109.0              19.0     160.0       66.0  \n",
       "1       131.0              15.0     153.0       64.0  \n",
       "2       128.0              14.0     120.0       85.0  \n",
       "\n",
       "[3 rows x 1004 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [str(i) for i in range(1, 1001)] + ['heart_rate', 'respiratory_rate', 'systolic', 'diastolic']\n",
    "total = pd.read_csv('total_large.csv', \n",
    "                     header=None, \n",
    "                     names=column_names)\n",
    "total.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "818593c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal2matrix(total):\n",
    "    total = total.values\n",
    "\n",
    "    numberOfLines = len(total)\n",
    "    returnMat = np.zeros((numberOfLines, 1003))\n",
    "    classLabelVector = []\n",
    "    index = 0\n",
    "\n",
    "    for line in total:\n",
    "        returnMat[index, :1002] = line[:1002]\n",
    "        returnMat[index, 1002] = line[1003]\n",
    "        if 100 <=line[1002]< 140:\n",
    "            classLabelVector.append(1)\n",
    "        elif 140 <=line[1002]<= 160:\n",
    "            classLabelVector.append(2)\n",
    "        index += 1\n",
    "    return returnMat, classLabelVector\n",
    "\n",
    "def autoNorm(dataSet):\n",
    "    minVals = dataSet.min(0)\n",
    "    maxVals = dataSet.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normDataSet = np.zeros(np.shape(dataSet))\n",
    "    m = dataSet.shape[0]\n",
    "    normDataSet = dataSet - np.tile(minVals, (m, 1))\n",
    "    normDataSet = normDataSet / np.tile(ranges, (m, 1))\n",
    "    return normDataSet, ranges, minVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea387857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix, class_labels = signal2matrix(total)\n",
    "norm_feature_matrix, ranges, minVals = autoNorm(feature_matrix)\n",
    "np.shape(feature_matrix)\n",
    "np.shape(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, \n",
    "                                                    class_labels, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce292f",
   "metadata": {},
   "source": [
    "### Using Stacking Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07de9d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9751111111111112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.98      0.98      2923\n",
      "           2       0.97      0.96      0.96      1577\n",
      "\n",
      "    accuracy                           0.98      4500\n",
      "   macro avg       0.97      0.97      0.97      4500\n",
      "weighted avg       0.98      0.98      0.98      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "base_learners = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('svm', SVC(random_state=42))\n",
    "]\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_learners, \n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_stack = stacking_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_stack))\n",
    "print(classification_report(y_test, y_pred_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c94b6",
   "metadata": {},
   "source": [
    "### Using Super learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e9921de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9751111111111112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.98      0.98      2923\n",
      "           2       0.97      0.96      0.96      1577\n",
      "\n",
      "    accuracy                           0.98      4500\n",
      "   macro avg       0.97      0.97      0.97      4500\n",
      "weighted avg       0.98      0.98      0.98      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "classifiers = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('svm', SVC(probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=classifiers, voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_voting))\n",
    "print(classification_report(y_test, y_pred_voting))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09115b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Mark J Van der Laan and Sherri Rose. _Targeted learning: causal inference for observational and experimental data._ Springer Science & Business Media, 2011.\n",
    "- Mark J Van der Laan, Eric C Polley, and Alan E Hubbard. _Super learner._ Statistical applications in genetics and molecular biology, 6(1), 2007."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
