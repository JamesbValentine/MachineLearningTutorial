{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d04368b",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Ruixuan Dong\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial is based on the context of Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Stanford, CA: Stanford University.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    " - [Adaboost.M1](#1)\n",
    " - [Boosting as a forward stagewise additive model](#2)\n",
    " - [Exponential loss](#3)\n",
    " - [Boosting trees](#4)\n",
    " - [Gradient boosting](#5)\n",
    " - [Tuning boosted trees](#6)\n",
    " - [Regularization](#7)\n",
    " - [Relative variable importance](#8)\n",
    " - [Simulated Dataset](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350eaab8",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "\n",
    "### 1 Adaboost.M1\n",
    "\n",
    "This is a procedure originally designed for classification problems, but can be extended to regression problems in a straightforward way. The idea of boosting is to combine many \"weak\" classifiers in order to construct a powerful \"committee\". In this way, boosting is like bagging (and consequently, random forests), but we will see that in fact, boosting is fundamentally different.\n",
    "\n",
    "To make matters concrete, let us return to the setting where $Y$ is a binary random variable which we encode by $Y \\in \\{-1, 1\\}$. Consider a set of predictors $X$ and a prediction function $G(X)$ taking values in $\\{-1, 1\\}$. The error rate on a training sample $\\{(y_i,x_i)\\}_{i=1}^n$ is given by\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\{y_i \\neq G(x_i)\\},\n",
    "$$\n",
    "and the expected error rate on future predictions is the expectation of $\\mathbf{1}\\{Y \\neq G(X)\\}$ with respect to the joint distribution of $X$ and $Y$.\n",
    "\n",
    "When we say we are combining weak classifiers, we mean classifiers which perform only slightly better than random guessing. In boosting, we repeatedly refit the classifier to a weighted version of the dataset. Then, we take a weighted average of these classifiers, where the weights are a function of the prediction accuracy. For example, if we let $G_m(x)$, $m = 1, \\ldots, M$ denote a sequence of weak classifiers fit to different versions of the sample data. Our final estimator would be a weighted majority vote estimator:\n",
    "$$\n",
    "G^{(M)}(X) = \\text{sign} \\left( \\sum_{m=1}^M \\alpha_m G_m(x) \\right),\n",
    "$$\n",
    "where the weights $\\alpha_1, \\ldots, \\alpha_M$ are computed in the process of constructing the $G_m$. Naturally, these weights can be interpreted in terms of the relative contribution of each classifier to the overall classifier. Intuitively, if $G_l$ has high classification accuracy, then $\\alpha_l$ should be large relative to the other $\\alpha_m$.\n",
    "\n",
    "Why boosting works well is a function of how the classifiers $G_m$ are constructed. As mentioned, each $G_m$ is computed based on a weighted version of the dataset. To compute $G_1$, we start with weights $\\frac{1}{n}$ assigned to each sample point. Then, we record misclassification accuracy and compute a new set of weights: upweighting points which are misclassified and downweighting those correctly classified. With this new weighted version of the dataset, we then fit $G_2$. This process is repeated until $M$ weak learners are obtained. A schematic taken from Hastie et al. (2009) can be found in Figure 1.\n",
    "\n",
    "<img src=\"figure1.png\" alt=\"Figure 1\" width=\"400\" height=\"400\">\n",
    "<p style=\"text-align:left;\">Figure 1: Schematic of the boosting procedure from Hastie et al. (2015).</p>\n",
    "\n",
    "Now, let us formally state a boosting algorithm: this particular version is known as AdaBoost.M1.\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:1px;border-width:0;color:black;background-color:black\">\n",
    "\n",
    "**Algorithm 1: AdaBoost.M1**\n",
    "\n",
    "1. Initialize the observation weights $w_i = \\frac{1}{n}$ for $i = 1, \\ldots, n$.\n",
    "\n",
    "2. For $m = 1, \\ldots, M$:\n",
    "\n",
    "    a. Fit classifier $G_m$ to the training data with weights $\\{w_i\\}_{i=1}^n$.\n",
    "    \n",
    "    b. Compute the classifier $G_m$'s weighted training error $\\varepsilon_m = \\sum_{i=1}^{n} w_i \\mathbf{1}\\{y_i \\neq G_m(x_i)\\}$.\n",
    "    \n",
    "    c. Compute $\\alpha_m = \\log\\left(\\frac{1-\\varepsilon_m}{\\varepsilon_m}\\right)$.\n",
    "    \n",
    "    d. Update weights $w_i \\leftarrow w_i \\exp[\\alpha_m \\mathbf{1}\\{y_i \\neq G_m(x_i)\\}]$.\n",
    "    \n",
    "3. Set $G(x) = \\text{sign}\\left\\{\\sum_{m=1}^{M} \\alpha_m G_m(x)\\right\\}$.\n",
    "\n",
    "\n",
    "<hr style=\"height:1px;border-width:0;color:black;background-color:black\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Boosting can improve the prediction accuracy of a \"weak\" classifier in somewhat surprising ways. Taking an example from Hastie et al. (2009), suppose we had predictors $X \\sim N_{10}(0, I_{10})$ and $Y$ deterministically defined by\n",
    "\n",
    "$$\n",
    "Y = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{if} \\sum_{j=1}^{10} X_j^2 > \\chi^2_{10}(0.5) \\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Then, with a training set of size $n = 2000$ and 10000 test cases, the AdaBoost.M1 algorithm is run using a \"stump\" – the cleverly named version of a regression tree with only a single split (i.e., a two-region partition). When applied naively to the training data, the testing error rate is nearly as bad as random guessing (45.8%), which should not be surprising. However, as the number of boosting iterations increases, the testing error starts to approach 5.8%. This can be compared to a full 244-node regression tree, which has a testing error around 25%. See Figure\n",
    "\n",
    "<img src=\"figure2.png\" alt=\"Figure 2\" width=\"400\" height=\"400\">\n",
    "<p style=\"text-align:left;\">Figure 2: Stump boosting example testing error from Hastie et al. (2009).\n",
    "</p>\n",
    "\n",
    "While somewhat intuitive – we are simply adjusting weights to put more importance on misclassified points – we have yet to touch upon the mechanism through which boosting works. Going forward we will:\n",
    "\n",
    "- Show that AdaBoost fits an additive model to a “base” learner so as to minimize an exponential loss function.\n",
    "- Show that the population minimizer of the exponential loss is the log-odds of the class probabilities.\n",
    "- Discuss alternative loss functions, and gradient boosting – a method for boosting trees under any loss function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30ab5f",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "### 2 Boosting as a forward stagewise additive model\n",
    "\n",
    "Examining the form of the boosting estimator,\n",
    "$$\n",
    "G(x) = \\text{sign}\\left\\{\\sum_{m=1}^M \\alpha_m G_m(x)\\right\\},\n",
    "$$\n",
    "it is not difficult to convince yourself that boosting is a method for fitting an additive expansion in a set of very elementary basis functions. Of course, here, the basis functions are individual classifiers $G_m$. Recall that basis expansions take the form\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^M \\beta_m b(x; \\gamma_m)\n",
    "$$\n",
    "where $\\beta_m$, $m = 1, \\ldots, M$ are the basis expansion coefficients and $b(x; \\gamma_m) \\in \\mathbb{R}$ are usually simple functions of the $p$-dimensional input variable $x$ (characterized by parameter set $\\gamma$). Some of the methods we are yet to discuss in this class can be characterized in this way.\n",
    "\n",
    "- For example, in single-hidden-layer neural networks, we take $b(x:\\gamma)=\\sigma(\\gamma_0 +\\gamma_1^\\prime x)$ where $\\sigma(t) = \\{1 + \\exp(-t)\\}^{-1}$ is the \"sigmoid\" function, and $\\gamma$ is a set of regression coefficients.\n",
    "- In a more familiar example, trees, $\\gamma$ represents the split variables and split points at the internal nodes, and the predictions are the terminal nodes.\n",
    "\n",
    "One helpful way to characterize boosting relates this method to forward stepwise modeling. To understand this connection, recall forward stepwise regression (which we mentioned only in passing): given a continuous response $y \\in \\mathbb{R}^n$ and predictors $X_1, \\ldots, X_p \\in \\mathbb{R}^n$, we\n",
    "\n",
    "1. Choose the predictor $X_j$ with smallest squared error loss $\\sum_{i=1}^n(y_i - \\hat{\\beta}_j X_{i,j})^2$.\n",
    "2. Choose the next predictor with smallest squared error loss $\\sum_{i=1}^n(r_i - \\hat{\\beta}_k X_{i,k})^2$ where $r_i = y_i - \\hat{\\beta}_j X_{i,j}$.\n",
    "3. Repeat Step 2.\n",
    "\n",
    "Returning to the characterization of boosting as an additive model, we can see that these models are fit by minimizing a loss function averaged over all the sample data, e.g.,\n",
    "\n",
    "$$\n",
    "\\min_{\\{\\beta_m, \\gamma_m\\}_{m=1}^M} \\sum_{i=1}^n L\\left(y_i, \\sum_{m=1}^M \\beta_m b(x_i; \\gamma_m)\\right). \\tag{1}\n",
    "$$\n",
    "\n",
    "This is often computationally expensive to solve, but if we apply the forward stagewise modeling approach, we can convince ourselves that as long as we can compute\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\gamma} \\sum_{i=1}^n L(y_i, \\beta b(x_i; \\gamma)),\n",
    "$$\n",
    "we can solve (1).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:1px;border-width:0;color:black;background-color:black\">\n",
    "\n",
    "**Algorithm 3**: Forward Stagewise Additive Modeling\n",
    "\n",
    "1. Initialize $f_0(x) = 0$.\n",
    "\n",
    "2. For $m = 1, \\ldots, M$:\n",
    "\n",
    "    a. Compute\n",
    "$$\n",
    "       (\\beta_m, \\gamma_m) = \\arg\\min_{\\beta, \\gamma} \\sum_{i=1}^n L\\{y_i, f_{m-1}(x_i) + \\beta b(x_i; \\gamma)\\}.\n",
    "$$\n",
    "       \n",
    "    b. Set $f_m(x) = f_{m-1}(x) + \\beta_m b(x; \\gamma_m)$.\n",
    "\n",
    "\n",
    "<hr style=\"height:1px;border-width:0;color:black;background-color:black\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "For example, were $L$ squared error loss, we see that the objective function in 2(a) can be written\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n L(y_i, f_{m-1}(x_i) + \\beta b(x_i; \\gamma)) = \\sum_{i=1}^n \\{y_i - f_{m-1}(x_i) - \\beta b(x_i; \\gamma)\\}^2\n",
    "= \\sum_{i=1}^n \\{r_i(m) - \\beta b(x_i; \\gamma)\\}^2\n",
    "$$\n",
    "\n",
    "where $r_i(m) = y_i - f_{m-1}(x_i)$ for $i = 1, \\ldots, n$.\n",
    "\n",
    "While this idea is helpful in terms of gaining intuition about boosting, it is still not clear what loss function $L$ AdaBoost.M1 is minimizing at each step. It turns out that AdaBoost.M1 (Algorithm 1) is equivalent to forward stagewise additive modeling (Algorithm 2) using the loss function\n",
    "\n",
    "$$\n",
    "L\\{y, f(x)\\} = \\exp\\{-yf(x)\\},\n",
    "$$\n",
    "\n",
    "which we will call the exponential loss function. We will discuss why this loss function is appropriate momentarily. For now, let us characterize the update from Step 2(a). Using the exponential loss, we see that 2(a) is\n",
    "\n",
    "$$\n",
    "(\\beta_m, G_m) = \\arg\\min_{\\beta, G} \\sum_{i=1}^n \\exp[-y_i\\{f_{m-1}(x_i) + \\beta G(x_i)\\}].\n",
    "$$\n",
    "\n",
    "\n",
    "Of course, we can write this problem as\n",
    "\n",
    "$$\n",
    "(\\beta_m, G_m) = \\arg\\min_{\\beta, G} \\sum w^{(m)}\\exp\\{-y_i\\beta G(x_i)\\}, \\tag{2}\n",
    "$$\n",
    "\n",
    "where weights will change at every iteration.\n",
    "\n",
    "To derive the solution to (2), notice that for any value of $\\beta$ (fixed),\n",
    "\n",
    "$$\n",
    "G_m = \\arg\\min_G \\sum w^{(m)} \\mathbf{1}\\{y_i \\neq G(x_i)\\},\n",
    "$$\n",
    "\n",
    "where $w^{(m)} = \\exp\\{-y_i f_{m-1}(x_i)\\}$. Since each $w^{(m)}$ does not depend on $\\beta$ or $G$, these can be regarded as weights for each observation. The minimizer is the classifier that minimizes the weighted error rate in classifying $y$.\n",
    "\n",
    "We can rewrite the criterion on\n",
    "\n",
    "$$\n",
    "e^{-\\beta} \\sum_{y_i = G(x_i)} w^{(m)} + e^{\\beta} \\sum_{y_i \\neq G(x_i)} w^{(m)}\n",
    "$$\n",
    "\n",
    "as\n",
    "\n",
    "$$\n",
    "(e^\\beta - e^{-\\beta})\\sum w^{(m)} \\mathbf{1}\\{y_i \\neq G(x_i)\\} + e^{-\\beta} \\sum w^{(m)}.\n",
    "$$\n",
    "\n",
    "Hence, with this $G_m$, we plug this back into (2) to solve with respect to $\\beta$. It follows that\n",
    "\n",
    "$$\n",
    "\\beta_m = \\frac{1}{2} \\log \\left(\\frac{1 - \\varepsilon_m}{\\varepsilon_m}\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\varepsilon_m = \\frac{\\sum w^{(m)} \\mathbf{1}\\{y_i \\neq G_m(x_i)\\}}{\\sum w^{(m)}}.\n",
    "$$\n",
    "\n",
    "Then, Step 2(b) of Algorithm 2 becomes\n",
    "\n",
    "$$\n",
    "f_m(x) = f_{m-1}(x) + \\beta_m G_m(x),\n",
    "$$\n",
    "\n",
    "so that the weights for the next iteration become\n",
    "\n",
    "$$\n",
    "w^{(m+1)} = w^{(m)} \\cdot \\exp\\{-\\beta_m y_i G_m(x_i)\\}.\n",
    "$$\n",
    "\n",
    "Based on these derivations, we can conclude that the AdaBoost.M1 algorithm is approximately minimizing the exponential loss function by fitting an additive model in a forward stagewise fashion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffcda44",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "### 3 Exponential loss\n",
    "\n",
    "Notably, the AdaBoost.M1 algorithm was not originally proposed with the exponential loss in mind. In fact, it was years after AdaBoost had been introduced that the connection between this procedure and the exponential loss was identified. However, this interpretation of the AdaBoost.M1 algorithm allows us to gain more insights and reveals ways this procedure can be generalized and improved.\n",
    "\n",
    "The exponential loss is attractive for this procedure particularly because it allows for simple, closed-form updates of the weights. But we must ask: what is the justification for minimizing the exponential loss? Clearly, it does not correspond to a particular likelihood, nor does it minimize misclassification accuracy on the training set (at least, in an explicit way). To address this question, we first consider the population level version of the exponential loss. Specifically, let\n",
    "\n",
    "$$\n",
    "f^*(x) = \\arg\\min_{f(x)} \\mathbb{E}_{Y|X=x}[\\exp\\{-Yf(x)\\}] = \\frac{1}{2} \\log \\frac{P(Y = 1 | X = x)}{P(Y = -1 | X = x)},\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "P(Y = 1 | X = x) = \\frac{\\exp\\{f^*(x)\\}}{\\exp\\{-f^*(x)\\} + \\exp\\{f^*(x)\\}} = \\frac{1}{1 + \\exp\\{-2f^*(x)\\}}.\n",
    "$$\n",
    "\n",
    "Hence, we can think of the additive expansion produced by the AdaBoost.M1 procedure as estimating one-half times the log-odds of $P(Y = 1 | X = x)$. This further justifies using the sign of the additive function as a classification rule.\n",
    "\n",
    "Let us compare this to the negative log-likelihood for a Bernoulli random variable. Let $p(x) = P(Y = 1 | X = x)$ and define $\\widetilde{Y} = (Y + 1)/2 \\in \\{0, 1\\}$. Then, we can write the negative log-likelihood in terms of $p(x)$:\n",
    "\n",
    "$$\n",
    "-L(Y, p(x)) = \\log[1 + \\exp\\{-2Yf(x)\\}].\n",
    "$$\n",
    "\n",
    "We know that the population log-likelihood is maximized at the true probabilities: $p(x)$. Hence, we see that because the minimizer of $[1 + \\exp\\{-2f^*(x)\\}]^{-1}$ is also the minimizer of $\\log[1 + \\exp\\{-2Yf(x)\\}]$, we see that at the population level, the exponential loss would give the same estimator as the negative log-likelihood. Of course, the exponential loss is not a log-likelihood and should not be interpreted as such.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972a288",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "### 4 Boosting trees\n",
    "\n",
    "Recall from our discussion of regression trees that we can express a tree as\n",
    "\n",
    "$$\n",
    "T(x; \\Theta) = \\sum_{j=1}^J \\gamma_j \\mathbf{1}(x \\in R_j)\n",
    "$$\n",
    "\n",
    "for regions $R_1, \\ldots, R_J$, which are defined by parameter set $\\Theta = \\{\\gamma_j, R_j\\}_{j=1}^J$. We estimate these parameters using\n",
    "\n",
    "$$\n",
    "\\hat{\\Theta} = \\arg\\min_{\\Theta} \\sum_{i=1}^n L(y_i, \\gamma_j).\n",
    "$$\n",
    "\n",
    "Of course, we saw that this was a fundamentally challenging problem for arbitrary $L$, so we relied on a greedy approximation using recursive binary partitioning. Let $T(x, \\Theta_m)$ denote a tree based on parameters $\\Theta_m$ estimated via recursive binary partitioning.\n",
    "\n",
    "Then, we can express a boosted tree model as a sum of trees\n",
    "\n",
    "$$\n",
    "f_M(x) = \\sum_{m=1}^M T(x, \\Theta_m)\n",
    "$$\n",
    "\n",
    "where each term is estimated in a sequential, stagewise manner. At each step in the forward stagewise procedure, one solves\n",
    "\n",
    "$$\n",
    "\\hat{\\Theta}_m = \\arg\\min_{\\Theta} \\sum_{i=1}^n L(y_i, f_{m-1}(x_i) + T(x_i, \\Theta)) \\tag{3}\n",
    "$$\n",
    "\n",
    "for the region set $\\{R_{jm}\\}_{j=1}^M$ and constants $\\{\\gamma_{jm}\\}_{j=1}^M$ defined by $\\Theta$, given model $f_{m-1}(x)$. Given regions $R_{jm}$, finding the optimal constants is straightforward: we solve a problem of the form\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}_{jm} = \\arg\\min_{\\gamma_{jm}} \\sum L(y_i, f_{m-1}(x_i) + \\gamma_{jm}).\n",
    "$$\n",
    "\n",
    "Computing the regions $R_{jm}$ is more difficult than it is for a single tree, but in a few special cases it is rather simple. For squared error loss, finding the regions and corresponding constants is no more difficult than for a single tree. It is simply a single tree fit to the residuals $y_i - f_{m-1}(x_i)$ for $i = 1, \\ldots, n$.\n",
    "\n",
    "For two-class classification and exponential loss, this stagewise approach yields exactly the AdaBoost procedure. Specifically, if the trees are restricted to be scaled classification trees (meaning $\\gamma_{jm} \\in \\{-1, 1\\}$), then, based on our discussion in Section 3.2, this is simply the tree that minimizes the weighted error rate where the weights are $w^{(m)} = \\exp\\{-y_i f_{m-1}(x_i)\\}$. When $\\gamma_{jm}$ is unrestricted, (3) still simplifies to a weighted exponential criterion for estimating a tree, which can be approximated using a recursive partitioning algorithm. Under exponential loss, it can be shown that\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}_{jm} = \\frac{1}{2} \\log \\frac{\\sum w^{(m)} \\mathbf{1}(y_i = 1)}{\\sum w^{(m)} \\mathbf{1}(y_i = -1)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat{\\Theta}_m = \\arg\\min_{\\Theta} \\sum w^{(m)} \\exp\\{-y_i T(x_i, \\Theta)\\}.\n",
    "$$\n",
    "\n",
    "Computing unrestricted trees is computationally intensive. For this reason, we might prefer simpler approximations based on, say, weighted least squares. In the following subsection, we will discuss an approach for approximating (3) under general loss functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5cd4f1",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "### 5 Gradient boosting\n",
    "\n",
    "In this section, we focus on minimizing criteria of the form\n",
    "\n",
    "$$\n",
    "L(f) = \\sum_{i=1}^n L\\{y_i, f(x_i)\\},\n",
    "$$\n",
    "\n",
    "where $f$ is restricted to be the sum of trees. Ignoring this constraint for a moment, we can see that the minimization of $L(f)$ can be viewed as the optimization problem\n",
    "\n",
    "$$\n",
    "\\hat{f} = \\arg\\min_f L(f)\n",
    "$$\n",
    "\n",
    "where $f \\in \\mathbb{R}^n$ are the values of the approximating function $f(x_i)$ at each of the $n$ sample points:\n",
    "\n",
    "$$\n",
    "f = \\{f(x_1), f(x_2), \\ldots, f(x_n)\\}'.\n",
    "$$\n",
    "\n",
    "One way to obtain $\\hat{f}$ is to solve the optimization as a sum of component vectors\n",
    "\n",
    "$$\n",
    "f_M = \\sum_{m=0}^M h_m, \\quad h_m \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "where $h_0$ denotes an initial guess, and each $f_m$ depends on $f_{m-1}$, the sum of previously induced updates. The question becomes how to compute each “increment” vector $h_m$.\n",
    "\n",
    "One such approach is steepest descent. Steepest descent sets $h_m = -\\rho_m g_m$ where $g_m$ is the gradient of $L(f)$ evaluated at $f = f_{m-1}$, and $\\rho_m$ is a scalar. The components of the gradient $g_m$ are\n",
    "\n",
    "$$\n",
    "g_{im} = \\left.\\frac{dL(y_i, f(x_i))}{df(x_i)}\\right|_{f(x_i)=f_{m-1}(x_i)},\n",
    "$$\n",
    "\n",
    "and the step length $\\rho_m$ is defined as\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\rho L(f_{m-1} - \\rho g_m).\n",
    "$$\n",
    "\n",
    "Hence, we obtain the approximate minimizer using\n",
    "\n",
    "$$\n",
    "f_m = f_{m-1} - \\rho_m g_m.\n",
    "$$\n",
    "\n",
    "This procedure can be compared to the forward stagewise boosting discussed previously. At each step of the forward stagewise boosting algorithm, we fit a tree that minimizes the loss given the current model $f_{m-1}$. Hence, we can think of those trees as analogous to the negative gradient above in gradient boosting. The main difference is that in the forward stagewise approach, the predictions' components are constrained to be coming from a tree whereas in gradient boosting, the negative gradient is the unconstrained maximal descent direction. However, there is a disconnect we need to resolve. Ultimately, we wanted to solve (3) (i.e., obtain a new tree conditional on $f_{m-1}$). While the negative gradient gives us the direction of steepest descent, it does not yield a solution to (3). A resolution is as follows: we let the negative gradient induce a tree whose predictions are as close as possible to the negative gradient at each step. Specifically, we solve (3) using\n",
    "\n",
    "$$\n",
    "\\tilde{\\Theta}_m = \\arg\\min_\\Theta \\sum \\left\\{-g_{im} - T(x_i, \\Theta)\\right\\}^2,\n",
    "$$\n",
    "\n",
    "which can be computed very efficiently. Although the solution regions to the above are not exactly the same as those from (3), they are generally similar. Then, with the regions, the functions in each region are obtained by the expression for $\\hat{\\gamma}_{jm}$ above. We summarize the gradient tree boosting algorithm for regression below.\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:1px;border-width:0;color:black;background-color:black\">\n",
    "\n",
    "**Algorithm 4**: Gradient Tree Boosting Algorithm\n",
    "\n",
    "1. Initialize $f_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma)$.\n",
    "\n",
    "2. For $m = 1, \\ldots, M$:\n",
    "\n",
    "    a. For $i = 1, \\ldots, n$ compute\n",
    "$$\n",
    "       r_{im} = \\left.\\frac{dL(y_i, f(x_i))}{df(x_i)}\\right|_{f=f_{m-1}}\n",
    "$$\n",
    "\n",
    "    b. Fit a regression tree to the targets $-r_{im}$. Let the terminal nodes of this tree be denoted $R_{jm}$ for $j = 1, \\ldots, J_m$.\n",
    "    \n",
    "    c. For $j = 1, \\ldots, J_m$, compute $\\hat{\\gamma}_{jm}$ as the solution to the problem\n",
    "$$\n",
    "       \\hat{\\gamma}_{jm} = \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{jm}} L(y_i, f_{m-1}(x_i) + \\gamma).\n",
    "$$\n",
    "\n",
    "    d. Update\n",
    "$$\n",
    "       f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{J_m} \\hat{\\gamma}_{jm} \\mathbf{1}(x \\in R_{jm}).\n",
    "$$\n",
    "\n",
    "3. Output $f(x) = f_M(x)$.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:1px;border-width:0;color:black;background-color:black\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4d513",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "### 6 Tuning boosted trees\n",
    "\n",
    "To implement boosting trees (or gradient boosting trees), one needs first to determine the size of the trees. Before, we argued that growing a large tree and pruning was the best strategy for estimating a single tree, but there, the context was different. This was the one and only tree to be used for prediction. In boosting, we are combining many trees, so overgrowing/pruning strategies are ill-advised. What is commonly done in practice is to simply set each tree to be of size $J$, where $J$ is treated as a model parameter to be tuned.\n",
    "\n",
    "In addition, one also needs to choose the number of boosting iterations $M$. Each iteration will further decrease the training risk $L(f_M)$, so that as we let $M \\rightarrow \\infty$, the training risk will tend towards zero. As we have seen in this course, this too is ill-advised as the fitted model will not generalize well to new data. Thus, we can think of there being some optimal number, $M^*$, of boosting iterations which minimize the risk on new samples. One strategy to choose $M$ is to use a validation set on which we measure prediction risk. We would then select the value of $M$ which minimizes this risk on the validation set. This can be thought of as an “early-stopping rule” since we are not allowing $L$ to be minimized on the training data by terminating our algorithm early.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326db0cb",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "### 7 Regularization\n",
    "\n",
    "Early stopping is not the only approach for mitigating overfitting as $M$ becomes large. As with ridge regression, and as we may discuss later in the course, neural networks, shrinkage techniques can also be applied. The simplest approach for shrinkage in the context of boosted trees is to scale the contribution of each tree by a factor of $\\nu \\in (0, 1)$ when it is added to the current approximation. That is, we would replace the update\n",
    "\n",
    "$$\n",
    "f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{J_m} \\gamma_{jm} \\mathbf{1}(x \\in R_{jm})\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "f_m(x) = f_{m-1}(x) + \\nu \\sum_{j=1}^{J_m} \\gamma_{jm} \\mathbf{1}(x \\in R_{jm}).\n",
    "$$\n",
    "\n",
    "The parameter $\\nu$ can be thought of as the learning rate of the boosting procedure. Smaller values of $\\nu$ lead to more shrinkage, and thus, result in a larger training risk for the same number of iterations $M$. In this sense, it is the combination of $\\nu$ and $M$ that control the prediction risk on the training data.\n",
    "\n",
    "Empirically, it has been found that values of $\\nu$ close to zero tend to perform better on out-of-sample prediction, but do require larger values of $M$. In fact, the best strategy appears to be setting $\\nu$ to be quite close to zero (say, $\\nu < 0.1$) and choose $M$ by early stopping. This often provides large improvements in prediction accuracy and probability estimation. The gains are less substantial in terms of classification accuracy, but apparent nonetheless.\n",
    "\n",
    "Another approach for regularization in boosting trees uses subsampling. In stochastic gradient boosting, at each iteration we fit the new tree using a subsample of the training observations. Typically, we select a subsample consisting of half of the training samples, although if $n$ is large, a smaller proportion can be used.\n",
    "\n",
    "Subsampling has two benefits. First, it reduces computing time, which is especially important when imposing the shrinking described above (which will inherently require larger $M$). Second, it often produces a more accurate model.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figure3.png\" alt=\"Figure 3\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae4281",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "### 8 Relative variable importance\n",
    "\n",
    "\n",
    "Like in random forests, it can be helpful to construct a metric for variable importance in boosted trees. For a single decision tree $T$, Breiman et al. (1984) proposed\n",
    "\n",
    "$$\n",
    "I_l^2(T) = \\sum_{t=1}^{J-1} \\hat{\\kappa}_t^2 \\mathbf{1}\\{v(t) = l\\}\n",
    "$$\n",
    "\n",
    "as a measure of importance for the variable $X_l$. The summation is taken over $J - 1$ internal (non-terminal) nodes of the tree. At each such node $t$, recall that one of the variables, say $v(t)$, was split into two regions at some point. This variable and split point were chosen to maximize the improvement, $\\hat{\\kappa}_t^2$, in square error risk over the model which fits a constant over the entire region. Thus, the variable importance measure defined above is the sum of all such squared improvements over all nodes for which the variable $X_l$ was chosen as the splitting variable.\n",
    "\n",
    "To generalize this idea to boosted trees (or random forests) is straightforward: we simply average the measure above over the trees. Specifically, define\n",
    "\n",
    "$$\n",
    "I_l^2 = \\frac{1}{M} \\sum_{m=1}^M I_l^2(T_m).\n",
    "$$\n",
    "\n",
    "For reasons discussed in Hastie et al. (2009), this metric is somewhat robust to high correlations in the predictors. It is important to point out that both of the measures introduced here are squared relative importance: the `randomForest` package (and others) report the square root of these quantities. Since these metrics are all relative, it is common to set the largest equal to 100 and rescale all others accordingly.\n",
    "\n",
    "For classification, a similar metric can be used. Supposing there are $K$ categories, $K$ separate models $f_k(x)$ are induced, each consisting of the sum of trees\n",
    "\n",
    "$$\n",
    "f_k(x) = \\sum_{m=1}^M T_{km}(x).\n",
    "$$\n",
    "\n",
    "Then, the squared importance of the $l$th variable for separating the $k$th category from all others is given by\n",
    "\n",
    "$$\n",
    "I_{lk}^2 = \\frac{1}{M} \\sum_{m=1}^M I_{lk}^2(T_{km}),\n",
    "$$\n",
    "\n",
    "where $I_{lk}^2(T_{km})$ is the squared importance measure for the $l$th variable in the $k$th model tree $T_{km}$.\n",
    "\n",
    "The overall importance of $X_l$ is then taken to be\n",
    "\n",
    "$$\n",
    "I_l^2 = \\frac{1}{K}\\sum_{k=1}^K I_{lk}^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be23c1f",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "### 9 Simulated Dataset\n",
    "\n",
    "In this part, we use the dataset generated by Synthetic SCG data generator https://github.com/wsonguga/DataDemo. In the data file, each row includes sensor data (10 seconds * 100Hz) + HeartRate + RespiratoryRate + SystolicBloodPressure + DiastolicBloodPressure.\n",
    "\n",
    "Now we would like to give a introduction about the synthetic SCG dataset we generated. \n",
    "\n",
    "Based on Synthetic SCG data generator, we generated an artificial (synthetic) scg signal of a given duration (10 seconds, i.e. duration=10) and sampling rate (100Hz, i.e. sampling rate=100) using a model based on Daubechies wavelets to roughly approximate cardiac cycles.\n",
    "\n",
    "Besides, we set \n",
    " - heart rate to be randomly chosen from the intgers range from 50 to 150, with the desired heart rate standard deviation (beats per minute) equal to 1.\n",
    " - respiratory rate to be randomly chosen from the intgers range from 10 to 30\n",
    " - diastolic blood pressure to be randomly chosen from the intgers range from 60 to 99\n",
    " - systolic blood pressure to be randomly chosen from the intgers range from 100 to 160\n",
    "\n",
    "The sample size of the current dataset is 6,000 in total.\n",
    "\n",
    "\n",
    "**Problem Statement:** The generated dataset containing: \n",
    "- a dataset set (\"lower.csv\") of 3,000 samples labeled as lower (100<=systolic blood pressure<140) \n",
    "- a dataset set (\"higher.csv\") of 3,000 samples labeled as higher (140<=systolic blood pressure<=160) \n",
    "- each sample is of shape (1, 1003) where 1003 is for the 1000-d signal and heart rate, respiratory rate and diastolic blood pressure\n",
    "\n",
    "In this part, we will build a simple kNN classifier that can correctly classify samples as lower or higher (SBP).\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afeb42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd3f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>1000</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.439234e-08</td>\n",
       "      <td>2.583753e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-4.897503e-07</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-2.029141e-07</td>\n",
       "      <td>3.029687e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.750468e-08</td>\n",
       "      <td>-3.504179e-08</td>\n",
       "      <td>-3.266654e-08</td>\n",
       "      <td>-2.969555e-08</td>\n",
       "      <td>-2.688206e-08</td>\n",
       "      <td>-2.599564e-08</td>\n",
       "      <td>109.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.781177e-08</td>\n",
       "      <td>3.850786e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-3.642447e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>8.308896e-07</td>\n",
       "      <td>-1.850758e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.937486e-08</td>\n",
       "      <td>-3.615418e-08</td>\n",
       "      <td>-3.250324e-08</td>\n",
       "      <td>-2.930146e-08</td>\n",
       "      <td>-2.813366e-08</td>\n",
       "      <td>-2.915194e-08</td>\n",
       "      <td>131.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.434446e-08</td>\n",
       "      <td>2.098668e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-1.939304e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-9.990558e-07</td>\n",
       "      <td>3.452373e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.199401e-08</td>\n",
       "      <td>-2.472291e-08</td>\n",
       "      <td>-1.890941e-08</td>\n",
       "      <td>-1.882332e-08</td>\n",
       "      <td>-2.188260e-08</td>\n",
       "      <td>-2.335538e-08</td>\n",
       "      <td>128.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1             2         3         4         5         6  \\\n",
       "0  5.439234e-08  2.583753e-07  0.000001  0.000004  0.000008  0.000006   \n",
       "1  5.781177e-08  3.850786e-07  0.000002  0.000007  0.000007 -0.000001   \n",
       "2  3.434446e-08  2.098668e-07  0.000003  0.000006 -0.000003  0.000002   \n",
       "\n",
       "              7         8             9            10  ...           995  \\\n",
       "0 -4.897503e-07 -0.000004 -2.029141e-07  3.029687e-06  ... -3.750468e-08   \n",
       "1 -3.642447e-06  0.000002  8.308896e-07 -1.850758e-06  ... -3.937486e-08   \n",
       "2 -1.939304e-06  0.000001 -9.990558e-07  3.452373e-07  ... -3.199401e-08   \n",
       "\n",
       "            996           997           998           999          1000  \\\n",
       "0 -3.504179e-08 -3.266654e-08 -2.969555e-08 -2.688206e-08 -2.599564e-08   \n",
       "1 -3.615418e-08 -3.250324e-08 -2.930146e-08 -2.813366e-08 -2.915194e-08   \n",
       "2 -2.472291e-08 -1.890941e-08 -1.882332e-08 -2.188260e-08 -2.335538e-08   \n",
       "\n",
       "   heart_rate  respiratory_rate  systolic  diastolic  \n",
       "0       109.0              19.0     160.0       66.0  \n",
       "1       131.0              15.0     153.0       64.0  \n",
       "2       128.0              14.0     120.0       85.0  \n",
       "\n",
       "[3 rows x 1004 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [str(i) for i in range(1, 1001)] + ['heart_rate', 'respiratory_rate', 'systolic', 'diastolic']\n",
    "total = pd.read_csv('total_large.csv', \n",
    "                     header=None, \n",
    "                     names=column_names)\n",
    "total.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b529aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal2matrix(total):\n",
    "    total = total.values\n",
    "\n",
    "    numberOfLines = len(total)\n",
    "    returnMat = np.zeros((numberOfLines, 1003))\n",
    "    classLabelVector = []\n",
    "    index = 0\n",
    "\n",
    "    for line in total:\n",
    "        returnMat[index, :1002] = line[:1002]\n",
    "        returnMat[index, 1002] = line[1003]\n",
    "        if 100 <=line[1002]< 140:\n",
    "            classLabelVector.append(1)\n",
    "        elif 140 <=line[1002]<= 160:\n",
    "            classLabelVector.append(2)\n",
    "        index += 1\n",
    "    return returnMat, classLabelVector\n",
    "\n",
    "def autoNorm(dataSet):\n",
    "    minVals = dataSet.min(0)\n",
    "    maxVals = dataSet.max(0)\n",
    "    ranges = maxVals - minVals\n",
    "    normDataSet = np.zeros(np.shape(dataSet))\n",
    "    m = dataSet.shape[0]\n",
    "    normDataSet = dataSet - np.tile(minVals, (m, 1))\n",
    "    normDataSet = normDataSet / np.tile(ranges, (m, 1))\n",
    "    return normDataSet, ranges, minVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85aa6498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix, class_labels = signal2matrix(total)\n",
    "norm_feature_matrix, ranges, minVals = autoNorm(feature_matrix)\n",
    "np.shape(feature_matrix)\n",
    "np.shape(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a703a996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.792\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.96      0.86      2923\n",
      "           2       0.88      0.47      0.61      1577\n",
      "\n",
      "    accuracy                           0.79      4500\n",
      "   macro avg       0.82      0.72      0.74      4500\n",
      "weighted avg       0.81      0.79      0.77      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, \n",
    "                                                    class_labels, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Create AdaBoost.M1 classifier\n",
    "ada_clf = AdaBoostClassifier(n_estimators=50, \n",
    "                             learning_rate=1.0, \n",
    "                             random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edcf2ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = ada_clf.feature_importances_\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9602a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495f9221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAGDCAYAAAAoD2lDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABbXUlEQVR4nO3dd5hURdbH8e+ZTBxyjkqUHBQwgxIUA5h4VURMqIiK7hrWsLtidlkDoiImFFwxEcwoZhEDGQQJEiRJjsMMk+r9oxoccBgGmJ473fP7PE8/3V03nb4TTlfdW1XmnENERESiT0zQAYiIiEh4KMmLiIhEKSV5ERGRKKUkLyIiEqWU5EVERKKUkryIiEiUUpIXCTGzX8zs1KDjKCrM7C4zezGgY48ysweCOHZBM7NLzezTw9xWv5NyRJTkpUgys+VmlmpmO83sj9A//dLhPKZzrplz7qtwHmMPM0s0s4fN7PfQ51xsZreZmRXG8XOJ51QzW5WzzDn3kHPu6jAdz8zsJjObZ2YpZrbKzN42sxbhON7hMrN/m9mYI9mHc+5151y3fBzrL19sCvN3UqKTkrwUZWc750oDrYE2wD+CDefQmVncARa9DZwGnAmUAS4DBgBPhSEGM7Oi9rf+FHAzcBNQAWgETAB6FvSB8vgZhF2QxxYBwDmnhx5F7gEsB07P8f4x4MMc7zsC3wNbgdnAqTmWVQBeAdYAW4AJOZadBcwKbfc90HL/YwI1gFSgQo5lbYCNQHzo/ZXAgtD+JwF1c6zrgBuAxcCyXD7baUAaUHu/8g5AFtAg9P4r4GHgJ2AbMHG/mPI6B18BDwJTQp+lAXBFKOYdwFLg2tC6pULrZAM7Q48awL+BMaF16oU+1+XA76FzcXeO45UAXg2djwXA7cCqA/xsG4Y+53F5/PxHAc8AH4bi/RE4Osfyp4CVwHZgOnBSjmX/Bt4BxoSWXw0cB0wNnau1wHAgIcc2zYDPgM3AOuAuoAeQDmSEzsns0LrJwEuh/awGHgBiQ8v6h875E6F9PRAq+y603ELL1od+pnOA5vgveBmh4+0E3t//7wCIDcX1W+icTGe/3yE99Nj/EXgAeuiR22O/f261gLnAU6H3NYFN+FpwDNA19L5yaPmHwJtAeSAeOCVU3jb0z7VD6B/m5aHjJOZyzC+Aa3LE8x9gROh1L2AJ0BSIA+4Bvs+xrgsljApAiVw+2yPA1wf43Cv4M/l+FUoizfGJ+F3+TLoHOwdf4ZNxs1CM8fha8tGhRHMKsAtoG1r/VPZLyuSe5F/AJ/RWwG6gac7PFDrntfDJ60BJ/jpgxUF+/qPwSfK4UPyvA2NzLO8LVAwt+xvwB5CUI+6M0M8pJhRvO/yXorjQZ1kADA6tXwafsP8GJIXed9j/HOQ49gTg+dDPpAr+S9ien1l/IBO4MXSsEuyb5Lvjk3O50M+hKVA9x2d+II+/g9vwfweNQ9u2AioG/beqR9F+FLUmPJGcJpjZDnyNbT3wr1B5X+Aj59xHzrls59xnwDTgTDOrDpwBXOec2+Kcy3DOfR3a7hrgeefcj865LOfcq/hE1TGXY/8PuBh8czfwf6EygGuBh51zC5xzmcBDQGszq5tj+4edc5udc6m57LsSPqnkZm1o+R6jnXPznHMpwL3ARWYWm9c5yLHtKOfcL865zNB5+NA595vzvgY+BU46QBwHcp9zLtU5NxvfetAqVH4R8FDonK8ChuWxj4p5fP6cxjnnfgqd49fxl20AcM6Ncc5tCn22/wKJ+OS3x1Tn3ITQuUl1zk13zv0QWn85PkmfElr3LOAP59x/nXNpzrkdzrkfcwvIzKrif78GO+dSnHPr8TXz/8ux2hrn3NOhY+3/88/Af4loAljodyg/5wJ8i8Q9zrmFoZ/hbOfcpnxuK8WUkrwUZb2cc2Xwtcwm/Jn86gIXmtnWPQ/gRKA6UBvY7Jzbksv+6gJ/22+72vim6f29A3QysxrAyfha7Lc59vNUjn1sxtesaubYfmUen2tjKNbcVA8tz20/K/A18krkfQ5yjcHMzjCzH8xsc2j9M9n3C0V+/JHj9S5gz82QNfY7Xl6ffxMH/vz5ORZm9jczW2Bm20KfJZl9P8v+n72RmX0QuolzO/6L2Z71a+ObwPOjLv5nsDbHeX8eX6PP9dg5Oee+wF8qeAZYZ2YjzaxsPo99KHGKAEryEgFCtc5RwNBQ0Up8Dbdcjkcp59wjoWUVzKxcLrtaCTy433YlnXNv5HLMrfia7kXAJcAbzjmXYz/X7refEs6573PuIo+PNBnoYGa1cxaa2XH4f+Rf5CjOuU4dfE1w40HOwV9iMLNEfHP/UKCqc64c8BH+y8nB4s2Ptfhm+tzi3t/nQC0za384BzKzk4A78D+b8qHPso0/Pwv89fM8B/wKNHTOlcVf296z/kr8ZYzc7L+flfjWn0o5zntZ51yzPLbZd4fODXPOtcNfSmmEb4Y/6HYHiVMkV0ryEimeBLqaWWv8DVVnm1l3M4s1s6RQF7BaoabPj4Fnzay8mcWb2cmhfbwAXGdmHUJ3nJcys55mVuYAx/wf0A84nz+b6gFGAP8ws2YAZpZsZhfm94M45ybjE927ZtYs9Bk64pukn3POLc6xel8zO8bMSgJDgHecc1l5nYMDHDYB36S9Acg0szOAnN261gEVzSw5v59jP2/hz0l5M6sJDDrQiqHP9yzwRijmhFD8/2dmd+bjWGXw1703AHFm9k/gYLXhMvib8HaaWRPg+hzLPgCqmdngUNfGMmbWIbRsHVBvT++E0O/Xp8B/zaysmcWY2dFmdgr5YGbHhn7/4oEU/A2YWTmOdVQem78I3G9mDUO/vy3NrGJ+jivFl5K8RATn3AbgNeBe59xK4Fx8bWwDvoZzG3/+Pl+Gr/H+ir+WPzi0j2n46/LD8XeBL8HfFHUg7+HvBF8Xuga9J5bxwKPA2FDT7zz8ddpDcT7wJfAJ/m7qMfg7tm/cb73R+FaMP/A3hd0UiuFg52AfzrkdoW3fwn/2S0Kfb8/yX4E3gKWhZujcLmHkZQiwCliGb6l4B1/jPZCb+LPZeiu+Gbo38H4+jjUJ/0VuEf4SRhp5Xx4A+Dv+M+/Af9l7c8+C0LnpCpyNP8+Lgc6hxW+HnjeZ2YzQ6374L03z8efyHfJ3+QH8l5EXQtutwF+62NNC9RJwTOj8T8hl28fxP79P8V9YXsLf2CdyQPZnC6SIFCVm9hX+zu5ARp07EmZ2PfB/zrl81XBFJDxUkxeRI2Zm1c3shFDzdWN8d7TxQcclUtxpNCYRKQgJ+LvM6+Ob38fir7uLSIDUXC8iIhKl1FwvIiISpZTkRUREolTEXZOvVKmSq1evXtBhiIiIFJrp06dvdM5VPtTtIi7J16tXj2nTpgUdhoiISKExsxWHs52a60VERKKUkryIiEiUUpIXERGJUkryIiIiUUpJXkREJEopyYuIiEQpJXkREZEopSQvIiISpZTkRUREopSSvIiISJQKW5I3s5fNbL2ZzTvAcjOzYWa2xMzmmFnbcMUiIiJSHIWzJj8K6JHH8jOAhqHHAOC5MMYiIiJS7IRtghrn3DdmVi+PVc4FXnPOOeAHMytnZtWdc2vDFZOIiBSi7CzYMBsyUkLvM2D1d7B1ScEfK20zrJgM5RoU/L4D9NHsatSrlHLY2wc5C11NYGWO96tCZX9J8mY2AF/bp06dOoUSnIiIHAbnYMcq2LwAvr0T1s8s3ONv+qVwjxcmKbvj+fsH3Rgx9Vha11gL/HBY+wkyyVsuZS63FZ1zI4GRAO3bt891HRERKUSZu2HrYtj8618fGfvVPGue+Ofr8o2hxvEQE4b0k50B5RtBUvmC33che/P15YyYOp34eOPiq7sya8jzh7WfIJP8KqB2jve1gDUBxSIiIgey9idY9jHs+gN2rITNC2HbUnDZua9fojJUaOIfrW+AKq0KN94o0P/GZsxcEsPVV7elVatq3DHk8PYTZJJ/DxhkZmOBDsA2XY8XESlC1v4I05+EhWP/usxi/PXvCk2gQtM/k3qFxlCiYqGHGukWLtzIDTd8xIsvnkO9euWIiTGefvrMI95v2JK8mb0BnApUMrNVwL+AeADn3AjgI+BMYAmwC7giXLGIiEg+ZWfC4vEw/QlYO9WXWaxP3B3uhjK1fJN7uQYQlxhsrFHAOcdzz03j73//lNTUTO6663P+97/zC2z/4by7/uKDLHfADeE6voiIHILd22DuizDzadi+wpclJkOLAdBmEJTVTc8Fbe3aHVx55Xt88onvbXDZZS15+ukzCvQYQTbXi4hI0Lb+BjOGwbyXIWOnLyvXANoOhmaXQ0LpQMOLVu++O58BAz5g8+ZUKlQowYgRPbnwwmYFfhwleRGR4sY5WPWNb5L/7T32dmyq3Rna3QJH9fTX3CUsVqzYysUXv0tGRjbdux/Nyy+fS40aZcJyLCV5EZHiIisdFr7pk/ue/uuxCdDkEmh7M1RpHWh4xUXduuV49NHTSUyM4/rr22OWW4/ygqEkLyIS7XZthDnPw6xnICXUialEZWh1PbS+HkpVCza+KLd7dyb33vslxx1XkwsuOAaAW27pVCjHVpIXEYlWm+b7LnALRkNmmi+r2Mw3yTe9FOKSAg2vOJgzZx19+45j7tz1VKlSijPPbEjJkvGFdnwleRGRaOIcrPjUN8kvn/Rnef0z/c10dU+HMDYPi5eVlc0TT/zA3Xd/QXp6Fg0aVGD06N6FmuBBSV5EJDpkpMKCMTDjSV+DB4gr4e+Qb3MzVGwSaHjFyYoVW7n88gl8/bXvinjdde0YOrQbpUolFHosSvIiIpFs51p/rX32CEjb5MtK14DWg6DlAI0+V8icc5x//ltMn76WqlVL8dJL59CzZ6PA4lGSFxGJROtmwown4NexfmIWgKrt/PX2Rhf6u+al0JkZTz99Bo8//gPPPnsmlSuXCjQeJXkRkUiRnQW/ve+T+6pvfJnFQMPzoO0tUPMEXW8PwMcfL+b771dy//1dAOjUqTZvv137IFsVDiV5EZGiLn0HzHsFZg7zI9QBJJSB5ldB25sguX6w8RVTKSnp3HbbZzz33DQAunY9mpNPrhtwVPtSkhcRKaq2r4AZT8O8F/3Y8gBl6/nE3vwqSCwbaHjF2Y8/ruKyy8azePFm4uNjuP/+zpxwQtGoveekJC8iUtSsmeq7wC1+988522ue6K+3H30uxMQGG18xlpGRxYMPfssDD3xDVpajWbPKjBlzHq1bF80BhZTkRUSKgqwMn9RnPOnncQeIiYPG/+eTe7X2gYYn3oMPfst9930NwK23duTBB08jKanoptKiG5mISHGQtgXmvOCneN25ypcllYeW10LrG/z87VJkDB7ckcmTlzJkSGe6dCn690IoyYuIBGHLYpjxlL+hLnOXLyvfGNoNhmMug/hgu16Jt3btDh566Fv+859uJCXFUa5cEt9+e0VYJ5UpSEryIiKFxTlY+aUfT37pB+yd4rXO6b5Jvn4PTfFahLz77nyuvfYDNm1KpWzZRB588DSAiEnwoCQvIhJ+mbvh1zf89fYNs31ZbKKfJKbtYKjcIsjoZD/btqVx002f8Npr/mfVrdvRDBx4bMBRHR4leRGRcNm13g83O+tZ2LXOl5Ws4q+1t7rOv5Yi5euvl3P55RNYsWIbSUlxDB3alYEDj42o2ntOSvIiIgVt4zzfBW7B65C125dVbulHpWtyMcQlBhuf5GratDV07vwqzkH79jUYPbo3TZpUCjqsI6IkLyJSEFw2LPvEJ/ffJ/9ZftRZ/np77c4acraIa9euOued15RjjqnMvfeeTHx85I9HoCQvInIkMlJg/mh/M92Whb4sriQ0vwLa3AQVgpuBTPKWlZXNk0/+QM+ejWjSpBJmxltvXUhMTPR8GVOSFxE5HDtWw6zhMOd539cdoHQtaHMjtLzG93WXIivnnO9jx/7Cjz9eTUyMRVWCByV5EZFD88c03yS/6C3IzvRl1Y7zTfINz4fY+GDjkzw55xg9eg433vgx27fvpkqVUvzrX6dEXXLfQ0leRORgsrPgt4k+ua/+zpdZjJ+3vd0tUKNTsPFJvmzcuIvrrvuAd99dAECvXk0YOfKswOd8DycleRGRA9m9Hea9BDOGwfblviyhLLS4BtreCGWL1rSicmDp6Vl06PAiS5duoXTpBIYN60H//q0jtmtcfinJi4jsb9syn9jnveTncgdIPgra3uxvqEsoE2x8csgSEmIZPLgDb775C6+91pujjioe90yYcy7oGA5J+/bt3bRp04IOQ0SijXOwegrMeAKWTPhzitdap/gm+aPO0hSvEebnn1ezatV2evduCkB2tsM5R2xs5A0dbGbTnXOHPBWhavIiUrxlpcOit30XuHWhCkRM/J9DzlZtG2R0chgyM7N56KFvGTLka0qWjKdNm+rUq1cudHNddDfP709JXkSKp9TNvvvbrOGwc40vS6roh5ttPRBK1wg2PjksixZt4rLLxvPTT6sBuOaatlSrVjrgqIKjJC8ixcumX2HmU/DLq5CZ6ssqHuNr7U37QnyJQMOTw+Oc4/nnp/O3v33Krl0Z1KpVlldf7RURc76Hk5K8iEQ/52DFZH+9fdnHf5bX6+6vt9ftpiFnI9wtt0ziqad+BODSS1swfPiZlCuXFHBUwVOSF5HolZnmJ4mZ8aSfNAYgLgmaXgbtBvsavESFyy9vxdix83jqqR706dM86HCKDCV5EYk+Kev89K6zn4PUDb6sVHU/xWvLa6FkZM8sJrB9+27efHMe11zTDoA2baqzbNnNlCihEQdzUpIXkeixfrZvkv/1DX/XPECVNr5JvnEfiE0INj4pEN98s4J+/cazYsU2kpOTuOiiZgBK8LlQkheRyOayYemHfsjZlV+GCg2OPtcn91on63p7lNi9O5N//vNL/vOf73HOTw3bsmXVoMMq0pTkRSQype/0d8jPfAq2LPZl8aWh+ZV+JrjyDYKNTwrUvHnr6dt3HLNnryMmxrj77hP55z9PiYo538NJSV5EIsv2laEpXkfC7q2+rEwdaHsTNL8KksoFGZ2EweTJS+nZ83+kp2dx9NHlGT26N5061Q46rIigJC8ikWHtj6EpXt8Bl+XLqncKTfHaG2L07yxadexYizp1kuncuR6PP96d0qV1b0V+6a9CRIqu7ExYPN4n97VTfZnFQuP/813gqncINDwJD+cc77wznzPOaEjp0gmULp3AtGnXkJysfu+HSkleRIqetK1+BriZT8P2Fb4ssRy0HACtB0FZNdVGq02bdnH99R/y9tvzufbadowYcRaAEvxhUpIXkaJj628w4ymY9wpk7PRl5RtCm5uh2eWQUHzHIC8OJk1awhVXTGTt2p2ULp3AccfVDDqkiKckLyLBcg5WfeOb5H97DwhNf12nix9P/qieYJE3Najk365dGdxxx2cMH/4zACecULtYzfkeTkryIhKMrHRY+KZP7utn+rLYBGhyiU/uVVoFGp4Ujq1b0+jY8UUWLtxEfHwMQ4Z05rbbjo/IOd+LIiV5ESlcuzbCnBEw6xlI+cOXlagMra6H1tdDqWrBxieFqly5JNq3r0FsbAxjxvSmTZvqQYcUVZTkRaRwbJoP05+EBaP9xDEAlZpD21ug6SV+4hgpFpYs2czu3Zk0a1YFgOee60lcXIyGpQ0DJXkRCR/nYMWnvkl++aQ/y+uf6fu31zlNQ84WI845XnhhBrfcMon69csxbdoAkpLiKFMmMejQopaSvIgUvIxUX2Of/iRsXuDL4kr4O+Tb3AwVmwQanhS+P/7YydVXv8eHH/ohiFu1qkZ6ehZJSUpD4aSzKyIFZ+daf6199ghI2+TLStf0fdtbDoASFYKNTwIxYcKvXHPN+2zcuIty5ZIYMaKn5nwvJEryInLk1s3wTfIL34TsDF9Wtb1vkm90IcTqWmtxdfPNHzNs2E8AnH76UYwadS41a5YNOKriQ0leRA5Pdhb89r6fv33VN77MYqDhef5mupon6Hq70Lp1NZKS4njssdO54YbjiInR70RhUpIXkUOTvsOPSDdzmB+hDiChDLS42k/xmlw/2PgkUOnpWUybtobjj/dDD/fv35ouXepTt265YAMrppTkRSR/tq+AGU/D3BcgfbsvS64PbW7yc7gnqgm2uNsz5/vChZuYOfNamjSphJkpwQcorEnezHoATwGxwIvOuUf2W54MjAHqhGIZ6px7JZwxicghcA7WTPVN8ovHgcv25TVP9Nfbjz4XYmKDjVECl53teOqpH/jHPz5n9+4sjjqqPCkp6UGHJYQxyZtZLPAM0BVYBfxsZu855+bnWO0GYL5z7mwzqwwsNLPXnXP67RAJUlYGLH7X30z3h79pipg4aHKxH3K2WvtAw5Oi4/fft9G//wS+/HI5AFdf3YbHH++uvu9FRDhr8scBS5xzSwHMbCxwLpAzyTugjJkZUBrYDGSGMSYRyUvaFpjzgp/idecqX5ZUAVpeC61vgDKaFUz+9PHHi7n44nfZtm03lSuX5MUXz+GccxoHHZbkEM4kXxNYmeP9KqDDfusMB94D1gBlgD7O7WkP/JOZDQAGANSpUycswYoUa5sX+SlefxkFmbt8WfnG0G4wHNMP4ksGGZ0UUfXrl2f37izOOacxL7xwNlWqlAo6JNlPOJN8bv0k3H7vuwOzgC7A0cBnZvatc277Phs5NxIYCdC+ffv99yEih8M5WPmlb5Jf+iF7/zzrdvXX2+t11xSv8hczZqylTZtqmBlNmlRixowBe2+wk6InnH/Bq4DaOd7XwtfYc7oCGOe8JcAyQONdioRT5m7fBW50a3j7NFj6gZ/itflVcPlcuOBTqH+GErzsY9euDG666WPatRvJq6/O3lvetGllJfgiLJw1+Z+BhmZWH1gN/B9wyX7r/A6cBnxrZlWBxsDSMMYkUnztWg+znoPZz/rXACWrQuuB0Oo6KFkl2PikyJo+fQ19+47n1183EhcXw9ataUGHJPkUtiTvnMs0s0HAJHwXupedc7+Y2XWh5SOA+4FRZjYX37x/h3NuY7hiEimWNsyFGU/Cgtcha7cvq9zSj0rX5GKI013QkrvMzGweeeQ77rvvazIzs2natBJjxpxH27aa8z1ShLWfvHPuI+Cj/cpG5Hi9BugWzhhEiiWXDcs+8dfbf58cKjQ46mx/vb32qRpyVvK0du0Ozj//LaZO9b0sbr65Aw8/fJrmfI8wGvFOJJpkpMAvr/k75bcs9GVxJaH5FdD2ZijfMNj4JGKUL1+C7dt3U7NmGUaN6sXppx8VdEhyGJTkRaLBjtUwazjMed73dQcoU9uPJd/iakgqH2x8EhHWrdtJYmIc5colkZQUx/jxfahUqSTly5cIOjQ5TEryIpHsj2m+SX7RW5AdGkeqegd/vb3heZriVfJt4kQ/53u3bkczZsx5ADRsWDHgqORIKcmLRJrsLFgywSf3NVN8mcX4edvb3QI1OgUankSWHTt2c8stk3jppZkA/PHHTlJTM3TtPUooyYtEit3bYd5LMGMYbF/uyxKTocU10GYQlK0baHgSeaZM+Z3LLhvPsmVbSUyM5dFHT+fGGztozvcooiQvUtRtW+YT+7yX/FzuAOWOhjY3Q/P+fi53kUPgnOOee77gkUemkJ3taNOmGmPGnMcxx1QOOjQpYEryIkWRc7D6O98k/9vEP6d4rXWKb5I/6ixN8SqHzczYtCkVgLvuOpF//etUEhL0+xSNlORFipKsdFj0tk/u66b7sph4aNrXT/FatU2g4Unkys52rF27g5o1ywLw3/924/LLW9GpU+2DbCmRTElepChI3QRzRvpucDtDUzwkVYTW10OrgVBaI4zJ4Vu5chv9+09kxYqtzJp1HaVLJ1CqVIISfDGgJC8SpE2/+iFn578Gmb75lIrH+Fp7074Qr/7JcmT+97+5DBz44d453xcu3Ei7djWCDksKiZK8SGFzDlZMhhlPwLKP/yyv18Nfb6/bVUPOyhHbvDmVG274iLFj5wFw1lmNePHFs6latXTAkUlhUpIXKSyZaX6SmBlPwkb/j5e4JDimnx9ytuIxgYYn0eOLL5bRr994Vq/eQalS8Tz5ZA+uuqqNpoQthpTkRcIt5Q+Y9SzMHgGpG3xZqerQ+gZoeS2UrBRsfBJ1Nm3axerVO+jUqRavvdabBg0qBB2SBERJXiRc1s/2TfK/vuHvmgeo0tY3yTe+CGITgo1PosqmTbuoWLEkABde2Izx42M566xGxMXFBByZBElJXqQguWxY+qHvArfyy1ChQYNePrnXPEnX26VAZWZm89hjU3jooW/57rsrad26GgC9ejUJODIpCpTkRQpC+k74ZZSf4nXrEl8WXxqaXwltb/Ij1IkUsN9+20y/fhP4/vuVAEyevHRvkhcBJXmRI7N9Jcx8Gua+ALu3+rKydaHNTdDiKj+2vEgBc87x0kszGTz4E1JSMqhRowyjRp1L1676Min7UpIXORxrfwxN8foOuCxfVuN43yTfoBfE6E9LwmP9+hSuueZ93ntvIQB9+jTj2Wd7UqGCxlSQv9J/IpH8ys6ExeN9cl871ZdZLDT+P5/cqx8XbHxSLOzalcGXXy4jOTmR557rycUXtwg6JCnClORFDiZtK8x90TfL7/jdlyWW893fWt8AZTU0qIRXSko6JUvGY2bUq1eOt966kGbNKlO7ti4HSd6U5EUOZOtv/ka6eS9DRoovK9/QDznb7HKILxVoeFI8TJnyO/36TeCWWzoyaJBvLerRo0HAUUmkUJIXyck5WPVNaIrX9wDny+uc5pP7UWeCqd+xhF96ehb33ffV3jnf/Rj0xxIToy6Ykn9K8iLgB6v5dawfcnb9TF8WmwBNLoV2g6FyyyCjk2Jm/vwN9O07jpkz/8AM7rzzBO67r7MSvBwyJXkp3nZthDkjYNYzfvhZgBKVofVAaHU9lKoabHxSrGRnO4YP/4k77phMWlom9eqVY/To3px4Yp2gQ5MIpSQvxdOm+TD9SVgw2k8cA1CpRWiK10v8xDEihSwrK5vRo+eQlpbJlVe25oknelC2bGLQYUkEU5KX4sM5WD7JX29f8emf5fXP9F3g6pymIWclELt3Z5KYGEd8fCxjxvRmwYKNGpZWCoSSvES/jFRfY5/+JGxe4MviSkCz/n6K1wqNg4xOirEtW/yc76mpmYwbdxFmRuPGlWjcWDMTSsFQkpfotXONv9Y++3lI2+TLSteE1oOg5QAooek3JTiff76U/v0nsmrVdkqVimfx4s00alQx6LAkyijJS/RZN8M3yS98E7IzfFm1Y6HtLdDoAoiNDzY+KdZSUzO4667PefLJHwHo2LEWo0drzncJDyV5iQ7ZWfDb+37+9lXf+DKLgYbn++vtNY7X9XYJ3MyZa+nbdzzz528gLi6Gf//7FO6440TN+S5hoyQvkS19B8x7xY9Mt22pL0soAy2uhjY3QnL9YOMTyeH11+cyf/4GmjSpxJgxvWnXrkbQIUmUU5KXyLRteWiK1xchfbsvS67vb6RrdgUklg00PJE9srKyiY31NfUHHuhC+fJJ3HJLJ0qW1GUjCT8leYkczsGaqb5JfvE4cNm+vOZJvkn+6HMgJjbYGEVCnHO88sos/vvfqUyZciXlyiWRlBTH3XefHHRoUowoyUvRl5UBi9/1N9P98ZMvi4mDJhf75F61XbDxiexn/foUBgx4n4kT/ZzvY8bM2Tu5jEhhUpKXoittC8wZCTOHw85VviypArS6DloNhDI1g41PJBfvv7+Qq69+n/XrU0hOTuSZZ87kkks057sEQ0leip7Ni/yNdL+MgsxdvqxCEz/k7DGXQXzJIKMTydXOnenceuskXnhhBgCdO9dj1Khe1KmjOd8lOEryUjQ4B79/4WeBW/rBn+V1u/om+XrdNcWrFGlTp67khRdmkJAQy8MPn8bgwR01a5wELt9J3sxKOedSwhmMFEOZu+HX//nkvmGOL4tN9DX2tjdDpeaBhieSF+ccFhp/oWvXo3n00dM588yGNG9eJeDIRLyDJnkzOx54ESgN1DGzVsC1zrmB4Q5Ootiu9TDrOZj9rH8NULIqtL7BX3MvWTnY+EQOYsGCDfTvP5Enn+xOp061Abj99hMCjkpkX/mpyT8BdAfeA3DOzTYz9QGRw7Nhrq+1L3gdsnb7ssqtfJN84/+DOE2rKUVbdrbjmWd+4vbb/Zzv9977JZMn9ws6LJFc5au53jm30vYdEjQrPOFIVHLZsOxj3wXu989Dheb7tbcdDLVP1ZCzEhFWr97OFVdM5LPP/OiK/fu35qmnegQclciB5SfJrww12TszSwBuAhaENyyJChkp8Mtr/k75Lb6/MPGl/Ih0bW+C8g2DjU/kELz55jyuv/5DtmxJo2LFEowceTbnndc06LBE8pSfJH8d8BRQE1gFfAroerwc2I7VMGs4zHne93UHKFPbjyXf4mpIKh9sfCKHaMuWVAYO/IgtW9I488yGvPTSOVSrVjrosEQOKj9JvrFz7tKcBWZ2AjAlPCFJxPrjZ98kv+htyM70ZdU7hKZ4Pd+PUicSgcqXL8ELL5zNhg0pDBjQDtPlJYkQ+fmv+zTQNh9lUhxlZ8GSCT65rwl977NYaHRRaIrXjoGGJ3I40tIyueuuz6lWrfTeO+bVNC+R6IBJ3sw6AccDlc3s1hyLygKaBaS4270d5r0EM4bB9uW+LDEZWlzjm+XL1gk0PJHDNWvWH/TtO45fftlAyZLxXHllGypV0iiLEpnyqskn4PvGxwFlcpRvBy4IZ1BShG1dCjOHwbyX/VzuAOWOhjY3Q/MrIEHXKSUyZWVlM3To99x775dkZGTTqFFFxozprQQvEe2ASd459zXwtZmNcs6tKMSYpKhxDlZ/55vkf5v45xSvtU/119uP6qkpXiWiLVu2hX79JvDdd78DcMMNx/LYY10157tEvPxck99lZv8BmgFJewqdc13CFpUUDVnp/ia66U/Auum+LCYemvb1/durtgk0PJGCMmjQx3z33e9Ur16al18+lx49GgQdkkiByE+Sfx14EzgL353ucmBDOIOSgKVu8t3fZj0DO9f4shKVoNX1/lG6erDxiRSwZ545k3/96ysef7wbFSuqeV6ihznn8l7BbLpzrp2ZzXHOtQyVfe2cO6VQItxP+/bt3bRp04I4dPTb9Ksfcnb+a5CZ6ssqHuOb5JteCvElAg1PpKC8//5C3nzzF157rbdmipOIEMrF7Q91u/zU5DNCz2vNrCewBqh1qAeSIso5WDEZZjzhh57do14P3wWublcNOStRY/853885pzEXXdQs4KhEwic/Sf4BM0sG/obvH18WGBzOoKQQZKT6SWJmPAmbfvFlcSXgmH5+iteK6hMs0WXq1JVcdtl4fvttCwkJsTz0UBcuuOCYoMMSCauDJnnn3Aehl9uAzrB3xLuDMrMe+CFxY4EXnXOP5LLOqcCTQDywMajLAMVGyh8w61mY/RykbvRlpapDm0HQ8looUTHY+EQKWEZGFkOGfM1DD31HdrajZcuqjBnTmxYtqgYdmkjY5TUYTixwEX7M+k+cc/PM7CzgLqAEkOet1aHtnwG64se8/9nM3nPOzc+xTjngWaCHc+53M6tyhJ9HDmT9bN8k/+sb/q55gCptQ1O8XgSxCcHGJxImL7wwgwce+BYzuP324xkypDOJiRpiWYqHvH7TXwJqAz8Bw8xsBdAJuNM5NyEf+z4OWOKcWwpgZmOBc4H5Oda5BBjnnPsdwDm3/pA/gRyYy4bfPvBN8iu/DBUaNOgN7QZDzZN0vV2i3jXXtOWLL5Zx443Hccop9YIOR6RQ5ZXk2wMtnXPZZpYEbAQaOOf+yOe+awIrc7xfBXTYb51GQLyZfYUfVe8p59xr++/IzAYAAwDq1NFwqQeVvhN+GeWneN26xJfFl4YWV0Gbm6DcUYGGJxJOq1dv57bbPuOpp3pQuXIp4uNjeeedi4IOSyQQeSX5dOf80GbOuTQzW3QICR4gtyri/v314oB2wGn4SwBTzewH59yifTZybiQwEnwXukOIoXjZvhJmPg1zX4DdW31Z2bo+sbe4yo8tLxLF3nrrF6677gO2bEkjISGWUaN6BR2SSKDySvJNzGxO6LUBR4feG+D29JnPwyp8c/8etfDd7/ZfZ6NzLgVIMbNvgFbAIiT/1vzgm+QXvQMuy5fVOME3yTfopSleJept3ZrGoEEf8frrcwE444wGPPzwaQFHJRK8vP77H2kfqp+BhmZWH1gN/B/+GnxOE4HhZhaHnxCnA/DEER63eMjOhMXj/JCza3/wZTFx0PhiP+Rs9eMCDU+ksHzxxTIuv3wCq1Ztp2TJeP77325ce63mfBeBvCeoOaJJaZxzmWY2CJiE70L3snPuFzO7LrR8hHNugZl9AswBsvHd7OYdyXGjXtpWmPuib5bf4SfTIKk8tBjgu8GV0ThFUnwsXbqFrl1Hk53tOO64mowe3ZtGjdQNVGSPgw5rW9QU22Fttyz5c4rXjBRfVr6RH7im2eUQXyrY+EQCcuedkylRIo677z6ZuLiYoMMRCYtwDmsrQXEOVn0dmuL1ffbet1jnNN+/vf4ZYPqnJsXHnjnf27evwWmn+V4ijzxyesBRiRRd+UryZlYCqOOcWxjmeAT8YDW/jvXJfcMsXxabAE0u9TfTVT7YPY8i0SfnnO+1a5dl0aIbSUpSPUUkLwf9CzGzs4Gh+Bvj6ptZa2CIc+6cMMdW/OzaCHNG+CleU0K9FUtUhtYD/RSvpTQMpxQ/zjlGjZrFTTd9ws6d6VSrVprnnz9LCV4kH/LzV/Jv/Oh1XwE452aZWb3whVQMbfzFd4FbMAYy03xZpRa+Sb7JxRCXFGh4IkHZsCGFa6/9gPHjfwXg/PObMmLEWVSqpDnfRfIjP0k+0zm3Td1RCphzsHySb5Jf8emf5Uf19PO31+miIWelWHPO0a3bGGbN+oOyZRMZPvwM+vZtqa5xIocgP0l+npldAsSaWUPgJuD78IYVxTJ2wfzRfsjZzQt8WVxJf4d825uhQuNg4xMpIsyMBx/swmOPTeHVV3tRt265oEMSiTgH7UJnZiWBu4FuoaJJwAPOubQwx5ariO1Ct3ONv9Y++3lI2+TLSteENjdCi2ugRIVg4xMpAn74YRXTpq1h0KA/B3Nyzqn2LsVeOLvQNXbO3Y1P9HKo1s3wTfIL34TsDF9W7VjfJN/oAoiNDzY+kSIgIyOL++//hgcf/BaADh1qcuyxNQGU4EWOQH6S/ONmVh14GxjrnPslzDFFvuws3699xhOw6htfZjE+qbcdDDWO1/V2kZBff91I377jmD59LWbwt791okUL9SQRKQgHTfLOuc5mVg24CBhpZmWBN51zD4Q9ukiTvsOPSDdjGGxb6ssSykKLq32zfHK9QMMTKUqcczzzzM/cdttnpKVlUqdOMq++2otTT60XdGgiUSNfHU1DU8wOM7MvgduBfwJK8ntsWx6a4vVFSN/uy5Lrh4acvQISywYankhRdN99X3PffV8D0K9fK4YN60FysrqLihSk/AyG0xToA1wAbALGAn8Lc1xFn3OwZqpvkl88Dly2L691sm+SP/ociIkNNESRomzAgHa88cY8HnywCxdccEzQ4YhEpfzU5F8B3gC6Oef2nw+++MnK8PO2z3gS/vjJl8XEQZNL/JCzVdsFGZ1IkbVtWxpPP/0T//jHicTGxlCjRhnmzx9IbKzmXxAJl/xck+9YGIEUeWlbYM5ImDkcdq7yZUkVoNV10PoGKF0j2PhEirCvvlpOv37jWblyO/HxMdxxx4kASvAiYXbAJG9mbznnLjKzueyd/swvApxzrnjMkrJ5kR+45pdRkLnLl1Vo4pvkj7kM4jW8psiBpKVlcs89X/D441NxDo49tga9ezcNOiyRYiOvmvzNoeezCiOQIiV9J6z53s/fvvTDP8vrdvPjydfrpileRQ5i9uw/6Nt3PPPmrSc21rj33pO5666TiI/XvSoiheWASd45tzb0cqBz7o6cy8zsUeCOv24VBaY9Dl//nb2NF7GJvsbedjBUahZkZCIR46efVnPiiS+TkZFNw4YVGD26Nx061Ao6LJFiJz833nXlrwn9jFzKosO8l9mb4I8f4q+5l6wcaEgikaZdu+p06lSbZs0q85//dKVUqYSgQxIplvK6Jn89MBA4yszm5FhUBpgS7sACk7rRP1+7BkpXDzYWkQjhnGP06Dl06VKfWrXKEhsbw6ef9iUxUXO+iwQpr7/A/wEfAw8Dd+Yo3+Gc2xzWqIKybblP8jFxkFAm6GhEIkLOOd9PP/0oJk3qS0yMKcGLFAF5/RU659xyM7th/wVmViEqE/2Mp8BlQZO+kFA66GhEirwPP1zEVVe9x7p1KZQpk8Bll7XUtAwiRcjBavJnAdPxF6lz/uk64KgwxlX4Zo+AWcP96/Ya0E8kLykp6fz9758yYsR0AE4+uS6vvtqLevXKBRuYiOwjr7vrzwo91y+8cAKQlQ5f3ARznvfvO9wFVVoHGpJIUbZ7dybHHvsCCxZsJCEhlgcf7MItt3TUwDYiRVB+xq4/AZjlnEsxs75AW+BJ59zvYY8u3Hath/cugNXf+q5y3V7w3eVE5IASE+O46KJmjBu3gDFjzqNlS00LK1JUmXMu7xX8nfWtgJbAaOAl4Dzn3CnhD++v2rdv76ZNm3bkO1o3Eyb2gh2/+yFpzxkP1Y878v2KRKGFCzeydu3OvdPAZmRkkZXlSErSzXUihcHMpjvn2h/qdvlpX8t0/pvAucBTzrmn8N3oItdv78PYE3yCr94RLp2mBC+SC+cczz77M23aPE+fPu+wfn0KAPHxsUrwIhEgP3+lO8zsH8BlwElmFgvEhzesMMpIgUlXQmaqn+v99OcgLjHoqESKnDVrdnDllROZNOk3AC68sBmJiRqSViSS5CfJ9wEuAa50zv1hZnWA/4Q3rDCaM9L3ha92HHR/CfX3Efmrd96Zz7XXfsDmzalUqFCC558/S3O+i0SggzbXO+f+AF4Hks3sLCDNOfda2CMLh8w0+Dn0/aTjvUrwIrm4887JXHjh22zenEqPHg2YN+96JXiRCHXQJG9mFwE/ARcCFwE/mtkF4Q4sLFZMhpS1UKkFHNUz6GhEiqQzzmhAqVLxPPvsmXz00SVUrx7Zt+CIFGf5aa6/GzjWObcewMwqA5OBd8IZWFhsX+GfaxyvWrxIyO7dmUya9BvnnNMYgFNOqceKFYOpWLFkwJGJyJHKz931MXsSfMimfG5X9Oxc7Z9L1ww2DpEiYs6cdRx77Auce+5YPv986d5yJXiR6JCfmvwnZjYJeCP0vg/wUfhCCiMleREAsrKyeeKJH7j77i9IT8+iQYMKlCmjXiYi0eagSd45d5uZnQeciB+/fqRzbnzYIwuHnWv8cxkleSm+VqzYyuWXT+Drr/3lq+uua8fQod0057tIFMprPvmGwFDgaGAu8Hfn3OrCCiwsMnb457hSwcYhEpAvv1xGr15vsn37bqpWLcXLL5/LmWc2DDosEQmTvK6tvwx8AJyPn4nu6UKJKFwyUmHDHP+6fINgYxEJSIsWVSlZMp7evZswb95AJXiRKJdXc30Z59wLodcLzWxGYQQUNqu/9aPcVWkDpaoFHY1Iofnqq+Ucf3xtEhJiqVSpJNOmXUONGmUw9TARiXp51eSTzKyNmbU1s7ZAif3eR5bln/jnej2CjUOkkKSkpHPDDR/SufOrDBny9d7ymjXLKsGLFBN51eTXAo/neP9HjvcO6BKuoMJiWSjJ11eSl+j300+rueyy8SxatIn4+BjKlUsKOiQRCcABk7xzrnNhBhJW21fA5gWQUAaqdwo6GpGwycjI4qGHvuX++78hK8vRvHkVxozpTatWukQlUhwVj7ki99Ti65wOsZE7gZ5IXjZvTuWMM17np598J5hbb+3Igw+epilhRYqx4vHXv+d6fP0zgo1DJIzKl0+iQoUS1K5dllGjetGlS/2gQxKRgEV/ks9Kh98/96/rdQ82FpECtnbtDtLTs6hbtxxmxquv9iIhIVbX4EUEyN8sdGZmfc3sn6H3dczsuPCHVkAWj4f0HVDxGChbJ+hoRArMuHELaNHiOS65ZBxZWdkAVKlSSgleRPbKz0QzzwKdgItD73cAz4QtooKUlQFT7vGv294cbCwiBWTbtjT695/A+ee/xaZNqZQuncCOHelBhyUiRVB+mus7OOfamtlMAOfcFjOLjEGu570EW5dA+UbQ/MqgoxE5Yt98s4J+/cazYsU2kpLiGDq0KwMHHqt+7yKSq/wk+Qwzi8X3jd8zn3x2WKMqCNt/h+9CtfgTHoCY6L/9QKLbvfd+wYMPfotz0L59DUaP7k2TJpWCDktEirD8NNcPA8YDVczsQeA74KGwRlUQvroV0jb5m+0aXRB0NCJHrGzZRMyMe+89me+/v1IJXkQOKj9Tzb5uZtOB0/BTzfZyzi0Ie2RHattS/3z8EFBTpkSg7GzHokWb9ibzW2/tRLduR2tgGxHJt/zcXV8H2AW8D7wHpITKira0Lf45MTnYOEQOw4oVWznttNfo1OklVq3aDkBsbIwSvIgckvxcqP4Qfz3egCSgPrAQaBbGuI5M+g7YvhxiE6Dc0UFHI5JvzjnGjJnDoEEfs337bqpUKcWKFVupVats0KGJSATKT3N9i5zvQzPQXRu2iArCxl/8c4WmuuFOIsamTbu47roPeeed+QD06tWEkSPPonLlUgFHJiKR6pAzoHNuhpkdG45gCszGef65UvNg4xDJp6++Ws4ll7zL2rU7KV06gWHDetC/f2t1jRORI3LQJG9mt+Z4GwO0BTbkZ+dm1gN4CogFXnTOPXKA9Y4FfgD6OOfeyc++87Rxrn+u1CLv9USKiBIl4li/PoUTT6zDa6/1on798kGHJCJRID81+TI5Xmfir9G/e7CNQn3rnwG6AquAn83sPefc/FzWexSYlN+gD2qTavJS9K1YsZW6dcsB0KFDLb76qj+dOtUiNjY/PVtFRA4uz/8moQRc2jl3X+jxoHPudedcWj72fRywxDm31DmXDowFzs1lvRvxXxrWH2rwB7RBNXkpujIzsxky5GsaNHia999fuLf8xBPrKMGLSIE64H8UM4tzzmXhm+cPR01gZY73q0JlOY9RE+gNjDjMY/zVrvWQugESykCZ2gW2W5GCsHjxJk488WX+9a+vyMzMZvbsdUGHJCJRLK/m+p/wCX6Wmb0HvA2k7FnonBt3kH3ndseQ2+/9k8AdzrmsvG4wMrMBwACAOnUO0kV/Ty2+YnMNgiNFhnOOkSOnc+utn7JrVwa1apXl1Vc157uIhFd+rslXADYBXfizv7wDDpbkVwE5q9K1gDX7rdMeGBtK8JWAM80s0zk3IedKzrmRwEiA9u3b7/9FYV97rsdXVlO9FA0bNqTQv/9EPvpoMQCXXtqC4cPP1JSwIhJ2eSX5KqE76+fxZ3LfI+9E6/0MNDSz+sBq4P+AS3Ku4JzbW40xs1HAB/sn+EOWsyYvUgQkJMQyb956ypdP4rnnetKnj343RaRw5JXkY4HS5K/Z/a8rOJdpZoPwd83HAi87534xs+tCywvuOnxOqslLEbB9+27i42MoUSKe5OQkxo27iGrVSlOzpkauE5HCk1eSX+ucG3IkO3fOfQR8tF9ZrsndOdf/SI4FwPaVsPZH/7pi0R11V6Lbt9+uoF+/CZx7bmOefLIHAO3a1Qg4KhEpjvLqrxN5d6293PDP1yUrBxeHFEu7d2dy552TOeWUUSxfvpUpU1aye3dm0GGJSDGWV03+tEKLoiA45yekydoNnZ8MOhopZubNW0/fvuOYPXsdMTHG3XefyL33nkJCQmzQoYlIMXbAJO+c21yYgRyxlD/87HNJ5aHtzUFHI8WEc44nnviBf/zjc9LTszj66PKMHt2bTp00RoOIBC96pmjbssg/l28cbBxS7Hz//UrS07MYMKAt//1vd0qXTgg6JBERIBqTfAUleQkv5xzbt+8mOTkJM2PEiLO44orW9OzZKOjQRET2ET0DZW8OjQFeXv9oJXw2b06lT5936Nz5VdLTswCoVKmkEryIFEnRk+TVXC9hNmnSEpo3f5a3357P4sWbmT37j6BDEhHJU/Qk+Z2r/HPZusHGIVFn164MbrzxI3r0eJ21a3dywgm1mT37Oo49tubBNxYRCVD0XJPfvdU/J5UPNAyJLtOmraFv33EsXLiJuLgYhgw5ldtvP0FTwopIRIieJJ+2xT8nKslLwfn559UsXLiJY46pzJgxvWnTpnrQIYmI5Ft0JHmXDbu3+deJycHGIhEvNTWDEiXiAbjuuvbExBj9+rXaWyYiEimio81x93bAQUJZiNEIY3J49sz5Xr/+U/z2mx8Lysy49tr2SvAiEpGiJMlv9c+J5YKMQiLYH3/s5Oyz3+Daaz9g3boUxo6dF3RIIiJHLDqa6/dcj9dNd3IYJkz4lWuueZ+NG3dRrpyf8/3//k9zvotI5IuOJL9nDvnS6tIk+bdjx24GD/6El1+eBcDppx/FK6+cS61amvNdRKJDdCT5paEp6+t1DzYOiSjLl29lzJi5JCXF8dhjp3PDDccRExN5MyyLiBxI5Cf57ExYMcm/rn9msLFIkZeZmU1cnL8VpUWLqrz88jm0aVOdY46pHHBkIiIFL/JvvFs83l+TL98YyjcIOhopwubNW0/79iP3uanu0ktbKsGLSNSK/CQ/bah/bjc40DCk6MrOdjzxxFTatx/J7NnrGDr0e5xzQYclIhJ2kd9cv22Zf27QK9AwpGhauXIb/ftP5Isv/O/J1Ve34fHHu2Oma+8iEv0iP8nvYZHfKCEFxznHG2/MY+DAD9m2bTeVK5fkxRfP4ZxzNEuhiBQf0ZPkRXJIT8/ivvu+Ztu23ZxzTmNeeOFsqlQpFXRYIiKFKvKTvMsOOgIpQrKzHTExRmJiHKNH92bOnHVcdVUbNc+LSLEU2UneZcPuPbPPlQs0FAlWamoGd945mdTUTEaOPBuA446ryXHHaYAkESm+IjvJp27yiT6pPMQmBB2NBGT69DX07TueX3/dSHx8DHfccQJHH10h6LBERAIX2Xer7Vrvn0tWDTYOCURmZjYPPvgNHTu+xK+/bqRp00r88MPVSvAiIiGRXZPftc4/l6wSbBxS6H77bTOXXTaeqVNXAXDzzR14+OHTNCWsiEgOEZ7kVZMvrh59dApTp66iZs0yjBrVi9NPPyrokEREipwIT/KqyRcnzrm9d8n/5z9dSUqK4777TqV8+RLBBiYiUkTpmrxEhAkTfuXUU18lNTUDgOTkJIYNO0MJXkQkD5Gd5FNUk492O3bs5qqrJtK795t8880KXnppZtAhiYhEjAhvrt9Tk1eSj0bfffc7/fqNZ9myrSQmxvLoo6czcOCxQYclIhIxIjvJp6q5Phqlp2fxr399yaOPTsE5aN26Gq+/fp6mhBUROUSR3Vyfttk/l6gYbBxSoD76aDGPPDIFM+Mf/ziRH3+8WgleROQwRHZNPn2Hf04oE2wcUqDOPbcxt9zSkfPOa8qJJ9YJOhwRkYgV2TX5PUk+vnSwccgRWblyGz17/o/58zcAYGY8/nh3JXgRkSMUuTX5lHWQkQJxJSBBST4S7T/ne2ZmNpMm9Q06LBGRqBG5ST4jxT+XrAoW2Q0SxdHmzakMHPghb775CwBnndWIF188O+CoRESiS+Qm+ax0/xyrscojzWef/cYVV0xk9eodlCoVzxNPdOfqq9tqzncRkQIWuUk+c5d/jisVbBxySDZt2kXv3m+SkpJBx461GD26Nw0aaNY4EZFwiNwkv6e5Pl5JPpJUrFiSJ57ozrp1Kdx554nExelSi4hIuERukt+x0j/rprsiLTMzm8cem0K1aqW58so2AFxzTbuAoxIRKR4iN8n//rl/rtgs2DjkgH77bTP9+k3g++9XUrp0Ar16NaFCBU0oIyJSWCI3ya+f5Z81pG2R45zjpZdmMnjwJ6SkZFCjRhleeeVcJXgRkUIWuUk+JhR6VTX9FiXr1u3kmmve5/33FwFw0UXNeO65nkrwIiIBiNwkv+fues1AV6T07TueyZOXkpycyLPP9uTii5ura5yISEAi99bmjFCSjy8ZbByyj//+txtnnNGAuXOv55JLWijBi4gEKHKTfPp2/6wudIGaMuV3brvt073vW7asykcfXUrt2skBRiUiIhCpzfUZuyB1I8QmqLk+IOnpWdx331c88sgUsrMdJ51Ul3POaRx0WCIikkNkJvntK/xz2boatz4A8+dvoG/fccyc+QdmcOedJ9C9+9FBhyUiIvuJzCS/bZl/Llsv0DCKm+xsx9NP/8gdd0xm9+4s6tUrx2uv9eKkk+oGHZqIiOQiMpP89uX+WUm+UA0f/hODB08C4MorW/PEEz0oWzYx4KhERORAIjPJ76nJJ9cPNo5i5qqr2vDWW7/wt791onfvpkGHIyIiBxHWC9pm1sPMFprZEjO7M5fll5rZnNDjezNrla8dqyZfKLZsSeWWWz5hx47dAJQqlcC3316hBC8iEiHCVpM3s1jgGaArsAr42czec87Nz7HaMuAU59wWMzsDGAl0OOjOVZMPu8mTl9K//wRWr97B7t1ZPPtsTwD1excRiSDhrMkfByxxzi11zqUDY4Fzc67gnPveObcl9PYHoFa+9rxtuX9OrlcwkcpeqakZDB78CV27jmb16h107FiLW2/tFHRYIiJyGMJ5Tb4msDLH+1XkXUu/Cvj4oHt12ZC2CeKSNDlNAZsxYy19+45jwYKNxMXF8O9/n8Idd2jOdxGRSBXOJJ9bu67LdUWzzvgkf+IBlg8ABgAcVae6LyxTB9R0XGAWL95Ex44vkpGRTePGFRkz5jzat68RdFgiInIEwpnkVwG1c7yvBazZfyUzawm8CJzhnNuU246ccyPx1+tp36KB/6JQpk4Bh1u8NWxYkUsvbUmZMgk88sjplCwZH3RIIiJyhMKZ5H8GGppZfWA18H/AJTlXMLM6wDjgMufconztNSvdP5dvVIChFj/OOV5+eSatW1ejXTtfY3/ppXOIiVHriIhItAhbknfOZZrZIGASEAu87Jz7xcyuCy0fAfwTqAg8G7prO9M51z5fB9Bwtodt/foUBgx4n4kTF9KkSSVmzbqWxMQ4JXgRkSgT1sFwnHMfAR/tVzYix+urgavDGYPs6/33F3L11e+zfn0KycmJ3HPPSSQkxAYdloiIhEFkjngnh2znznRuvXUSL7wwA4DOnesxalQv6tTRlLAiItFKSb4YyM52nHLKKGbMWEtiYiwPP3waN9/cUc3zIiJRThe2i4GYGOOWWzrSqlVVpk0bwC23dFKCFxEpBpTko9SCBRsYO3be3veXXtqCn3++hubNqwQYlYiIFCY110eZ7GzH8OE/cccdk3HO0aJFFZo1q4KZER+vG+xERIoTJfkosmrVdq64YiKTJy8F4IorWlO7tm6sExEprpTko8Sbb87juus+ZOvWNCpVKsnIkWdpSlgRkWJOST4KPPDAN9x775cA9OzZkBdfPIdq1UoHHJWIiARNN95FgT59mlGlSilGjOjJ++9frAQvIiKAavIRKS0tkzFj5nDVVW0wMxo2rMiyZTdrUhkREdmHknyEmTlzLX37jmf+/A0AXH11WwAleBER+YvIa653Wf45LinYOApZVlY2jzzyHR06vMj8+Rto3LgirVpVDTosEREpwiKvJp+Z5p8rHhNsHIVo6dIt9Os3nilTVgIwaNCxPPpoV9XeRUQkT5GX5LN2++fko4KNo5D89NNqTjvtNXbuTKd69dK88sq5dO/eIOiwREQkAkReks8ONdeXqBhsHIWkVauqHHVUeRo3rshzz/WkYsWSQYckIiIRIvKSvMv0z4nlg40jjD76aDEdO9aiQoUSJCbG8fXX/UlOTsRMk8qIiEj+Rd6Nd3tq8knRl+R37kxnwID36dnzfwwc+CHOOQDKlUtSghcRkUMWeTV5HMQmQFyJoAMpUFOnruSyy8bz229bSEiI5dhjawQdkoiIRLgITPJAfCmIkpptRkYWQ4Z8zUMPfUd2tqNly6qMGdObFi3UPU5ERI5MZCb5KKnFp6VlctJJrzBt2hrM4Pbbj2fIkM4kJkbmj0VERIqWyMwmUZLkk5Li6NChJhs2pPDaa705+eS6QYckIiJRJPJuvIOITvKrV29n1qw/9r5/7LGuzJlzvRK8iIgUuMhM8hYbdASH5a23fqFFi+c477w32bHDD+pTsmQ8ZcsmBhyZiIhEo8hM8hFm69Y0+vYdR58+77BlSxpNm1YmLS0z6LBERCTKReg1+cgZ9e3LL5dx+eUTWLlyOyVLxvP4490YMKCd+r2LiEjYRWaSTygddAT5cv/9X/PPf34FQIcONRk9ujcNGxaP4XhFRCR4kdlcn1Am6AjypVmzKsTFxTBkyKl8992VSvAiIlKoIrMmH180a/JZWdn89NNqOnWqDcB55zVl0aJB1K8ffUPwiohI0aeafAFZvnwrnTu/ysknj2L69DV7y5XgRUQkKJFZky9CSd45x6uvzuammz5mx450qlUrzY4d6UGHJSIiEqFJPiYh6AgA2LAhhWuv/YDx438F4Pzzm/L882dpzncRESkSIjPJFwFTpvzO+ee/xbp1KZQtm8jw4WfQt29LdY0TEZEiQ0n+MNWunUxqaiannFKXV1/tRd265YIOSUREZB9K8odg7tx1NGtWhZgYo06dZL7//kqaNKlEbGxk3r8oIiLRTUk+HzIysnjggW948MFvGTq0G4MHdwR8P3gRiU4ZGRmsWrWKtLS0oEORYiQpKYlatWoRHx9fIPtTkj+IhQs30rfv+L1zvm/atCvokESkEKxatYoyZcpQr1493WsjhcI5x6ZNm1i1ahX169cvkH0qyR+Ac45nn/2Z2277jNTUTOrUSea113pxyin1gg5NRApBWlqaErwUKjOjYsWKbNiwocD2qSSfiy1bUrn44neZNOk3APr1a8WwYT1ITk4KODIRKUxK8FLYCvp3Tkk+F2XKJLJ1axoVKpTg+efP4oILjgk6JBERkUOm28JDtm1LY+NGf709Li6GsWMvYN6865XgRSQwsbGxtG7dmubNm3P22WezdevWvct++eUXunTpQqNGjWjYsCH3338/zrm9yz/++GPat29P06ZNadKkCX//+98D+AR5mzlzJldffXXQYRzQ7t276dOnDw0aNKBDhw4sX778L+vs2LGD1q1b731UqlSJwYMHA7BixQpOO+00WrZsyamnnsqqVasA2LBhAz169CiUz6AkD3z11XJathzBFVdM3PtHUq9eOapXLzrD54pI8VOiRAlmzZrFvHnzqFChAs888wwAqampnHPOOdx5550sWrSI2bNn8/333/Pss88CMG/ePAYNGsSYMWNYsGAB8+bN46ijjirQ2DIzM494Hw899BA33nhjoR7zULz00kuUL1+eJUuWcMstt3DHHXf8ZZ0yZcowa9asvY+6dety3nnnAfD3v/+dfv36MWfOHP75z3/yj3/8A4DKlStTvXp1pkyZEvbPUKyb69PSMrnnni94/PGpOAdVq5Zi27bdlCuna+8iksN/w3Rt/m/u4OuEdOrUiTlz5gDwv//9jxNOOIFu3boBULJkSYYPH86pp57KDTfcwGOPPcbdd99NkyZNAIiLi2PgwIF/2efOnTu58cYbmTZtGmbGv/71L84//3xKly7Nzp07AXjnnXf44IMPGDVqFP3796dChQrMnDmT1q1bM378eGbNmkW5cuUAaNCgAVOmTCEmJobrrruO33//HYAnn3ySE044YZ9j79ixgzlz5tCqVSsAfvrpJwYPHkxqaiolSpTglVdeoXHjxowaNYoPP/yQtLQ0UlJSeP/997nxxhuZO3cumZmZ/Pvf/+bcc89l+fLlXHbZZaSkpAAwfPhwjj/++Hyf39xMnDiRf//73wBccMEFDBo0COfcAa+bL168mPXr13PSSScBMH/+fJ544gkAOnfuTK9evfau26tXL15//fW/nJeCVmyT/Jw56+jbdxxz564nNta4556Tufvuk4iPjw06NBGRfWRlZfH5559z1VVXAb6pvl27dvusc/TRR7Nz5062b9/OvHnz+Nvf/nbQ/d5///0kJyczd+5cALZs2XLQbRYtWsTkyZOJjY0lOzub8ePHc8UVV/Djjz9Sr149qlatyiWXXMItt9zCiSeeyO+//0737t1ZsGDBPvuZNm0azZs33/u+SZMmfPPNN8TFxTF58mTuuusu3n33XQCmTp3KnDlzqFChAnfddRddunTh5ZdfZuvWrRx33HGcfvrpVKlShc8++4ykpCQWL17MxRdfzLRp0/4S/0knncSOHTv+Uj506FBOP/30fcpWr15N7dp+6vC4uDiSk5PZtGkTlSpVyvXcvPHGG/Tp02fvl4BWrVrx7rvvcvPNNzN+/Hh27NjBpk2bqFixIu3bt+eee+456Pk+UsUyyf/3v99z111fkJ6eRcOGFRg9ujcdOtQKOiwRKaoOocZdkFJTU2ndujXLly+nXbt2dO3aFSDP2uSh3J09efJkxo4du/d9+fIHnxr7wgsvJDbWV4b69OnDkCFDuOKKKxg7dix9+vTZu9/58+fv3Wb79u3s2LGDMmX+vAS6du1aKleuvPf9tm3buPzyy1m8eDFmRkZGxt5lXbt2pUKFCgB8+umnvPfeewwdOhTwXR1///13atSowaBBg5g1axaxsbEsWrQo1/i//fbbg37GPXLe47BHXud37NixjB49eu/7oUOHMmjQIEaNGsXJJ59MzZo1iYvzabdKlSqsWbPmQLsqMJGZ5Ms3OKLN169PIT09i+uvb89//tOVUqWKxqx2IiI57bkmv23bNs466yyeeeYZbrrpJpo1a8Y333yzz7pLly6ldOnSlClThmbNmjF9+vS9TeEHcqAvCznL9h/xr1SpUntfd+rUiSVLlrBhwwYmTJiwt2aanZ3N1KlTKVGiRJ6fLee+7733Xjp37sz48eNZvnw5p556aq7HdM7x7rvv0rhx43329+9//5uqVasye/ZssrOzSUrK/bLrodTka9WqxcqVK6lVqxaZmZls27Zt75eN/c2ePZvMzMx9Wlhq1KjBuHHjAH9p5N133yU5ORnw5zWv81NQIvPGu4qHdse7c461a//8oQ4Z0pnPP+/Hs8/2VIIXkSIvOTmZYcOGMXToUDIyMrj00kv57rvvmDx5MuBr/DfddBO33347ALfddhsPPfTQ3tpsdnY2jz/++F/2261bN4YPH773/Z7m+qpVq7JgwYK9zfEHYmb07t2bW2+9laZNm1KxYsVc9ztr1qy/bNu0aVOWLFmy9/22bduoWbMmAKNGjTrgMbt3787TTz+9t5Y9c+bMvdtXr16dmJgYRo8eTVZWVq7bf/vtt/vcKLfnsX+CBzjnnHN49dVXAX9vQpcuXQ5Yk3/jjTe4+OKL9ynbuHEj2dnZADz88MNceeWVe5ctWrRon8sV4RKZSf4QbNy4iwsueJv27V9g8+ZUABIT4+jSpWCGDBQRKQxt2rShVatWjB07lhIlSjBx4kQeeOABGjduTIsWLTj22GMZNGgQAC1btuTJJ5/k4osvpmnTpjRv3py1a9f+ZZ/33HMPW7ZsoXnz5rRq1Yovv/wSgEceeYSzzjqLLl26UL169Tzj6tOnD2PGjNnbVA8wbNgwpk2bRsuWLTnmmGMYMWLEX7Zr0qQJ27Zt21urvv322/nHP/7BCSeccMAEDb7Gn5GRQcuWLWnevDn33nsvAAMHDuTVV1+lY8eOLFq0aJ/a/+G66qqr2LRpEw0aNODxxx/nkUce2busdevW+6z71ltv/SXJf/XVVzRu3JhGjRqxbt067r777r3LvvzyS3r27HnEMR6M5XbNoShrX9vctGnToGq7g6770UeLufLKiaxbl0KZMgm8997FnHpqvfAHKSIRb8GCBTRt2jToMKLaE088QZkyZYp0X/lwOfnkk5k4cWKu90Hk9rtnZtOdc+0P9ThRWZNPSUnn+us/oGfP/7FuXQonnVSHOXOuV4IXESlCrr/+ehITE4MOo9Bt2LCBW2+9NV83Oh6pyLzxLg8//7yaSy8dx+LFm4mPj+HBB7tw662dNOe7iEgRk5SUxGWXXRZ0GIWucuXK+/SZD6eoS/KbN6eyePFmmjevwpgxvWnVqlrQIYlIhMqrq5pIOBT0JfSoSPJbtqRSvrzvitC9ewPGjbuIM85oSFJSVHw8EQlAUlLS3oFLlOilMOyZT/5A3f8OR0RnQecczz03jTvumMzHH1/KiSfWAaB3b90sIyJHplatWqxatapA5/YWOZikpCRq1Sq4wdnCmuTNrAfwFBALvOice2S/5RZafiawC+jvnJuRn32vXbuDK698j08+8f0sP/xw0d4kLyJypOLj46lfX11tJbKFLcmbWSzwDNAVWAX8bGbvOefm51jtDKBh6NEBeC70nKd3P1jLgNs/Z/PmVCpUKMGIET258MJmBf8hREREIlg4a/LHAUucc0sBzGwscC6QM8mfC7zm/J0GP5hZOTOr7pz766gNIcs3l+OCq6cD0L370bz88rnUqKEpYUVERPYXzn5lNYGVOd6vCpUd6jr72JaWSIkSMQwffgYff3ypEryIiMgBhLMmn9vtqPv3DcjPOpjZAGBA6O3uzNR/zhs06J+ERnCUglcJ2Bh0EMWAznP46RyHn85x4Wh88FX+KpxJfhVQO8f7WsD+8+rlZx2ccyOBkQBmNu1whvaT/NM5Lhw6z+Gncxx+OseFw8ymHc524Wyu/xloaGb1zSwB+D/gvf3WeQ/oZ15HYFte1+NFREQk/8JWk3fOZZrZIGASvgvdy865X8zsutDyEcBH+O5zS/Bd6K4IVzwiIiLFTVj7yTvnPsIn8pxlI3K8dsANh7jbkQUQmuRN57hw6DyHn85x+OkcF47DOs8RN9WsiIiI5I+mZhMREYlSRTbJm1kPM1toZkvM7M5clpuZDQstn2NmbYOIM5Ll4xxfGjq3c8zsezNrFUSckexg5zjHeseaWZaZXVCY8UWL/JxnMzvVzGaZ2S9m9nVhxxjp8vH/ItnM3jez2aFzrHusDpGZvWxm681s3gGWH3rec84VuQf+Rr3fgKOABGA2cMx+65wJfIzva98R+DHouCPpkc9zfDxQPvT6DJ3jgj/HOdb7An//ygVBxx1pj3z+LpfDj7ZZJ/S+StBxR9Ijn+f4LuDR0OvKwGYgIejYI+kBnAy0BeYdYPkh572iWpPfOySucy4d2DMkbk57h8R1zv0AlDOz6oUdaAQ76Dl2zn3vnNsSevsDfhwDyb/8/B4D3Ai8C6wvzOCiSH7O8yXAOOfc7wDOOZ3rQ5Ofc+yAMqGJx0rjk3xm4YYZ2Zxz3+DP24Ecct4rqkk+LEPiyj4O9fxdhf8GKfl30HNsZjWB3sAI5HDl53e5EVDezL4ys+lm1q/QoosO+TnHw4Gm+AHN5gI3O+eyCye8YuOQ815RnU++wIbElQPK9/kzs874JH9iWCOKPvk5x08CdzjnsnwFSA5Dfs5zHNAOOA0oAUw1sx+cc4vCHVyUyM857g7MAroARwOfmdm3zrntYY6tODnkvFdUk3yBDYkrB5Sv82dmLYEXgTOcc5sKKbZokZ9z3B4YG0rwlYAzzSzTOTehUCKMDvn9f7HROZcCpJjZN0ArQEk+f/Jzjq8AHnH+4vESM1sGNAF+KpwQi4VDzntFtbleQ+KG30HPsZnVAcYBl6nGc1gOeo6dc/Wdc/Wcc/WAd4CBSvCHLD//LyYCJ5lZnJmVBDoACwo5zkiWn3P8O76lBDOrip9QZWmhRhn9DjnvFcmavNOQuGGXz3P8T6Ai8GyoppnpNBFFvuXzHMsRys95ds4tMLNPgDlANvCicy7XbkryV/n8Xb4fGGVmc/HNync45zQ73SEwszeAU4FKZrYK+BcQD4ef9zTinYiISJQqqs31IiIicoSU5EVERKKUkryIiEiUUpIXERGJUkryIiIiUUpJXiQAoRnnZuV41Mtj3Z0FcLxRZrYsdKwZZtbpMPbxopkdE3p9137Lvj/SGEP72XNe5oVmNCt3kPVbm9mZBXFskWikLnQiATCznc650gW9bh77GAV84Jx7x8y6AUOdcy2PYH9HHNPB9mtmrwKLnHMP5rF+f6C9c25QQcciEg1UkxcpAsystJl9HqplzzWzv8xWZ2bVzeybHDXdk0Ll3cxsamjbt83sYMn3G6BBaNtbQ/uaZ2aDQ2WlzOzD0Lzg88ysT6j8KzNrb2aPACVCcbweWrYz9Pxmzpp1qAXhfDOLNbP/mNnP5ufBvjYfp2Uqock3zOw4M/vezGaGnhuHRl4bAvQJxdInFPvLoePMzO08ihQnRXLEO5FioISZzQq9XgZcCPR2zm03s0rAD2b2ntu3qe0SYJJz7kEziwVKhta9BzjdOZdiZncAt+KT34GcDcw1s3b4EbM64Eco+9HMvsbPGb7GOdcTwMySc27snLvTzAY551rnsu+xQB/go1ASPg24Hj/B0Tbn3LFmlghMMbNPnXPLcgsw9PlOA14KFf0KnBwaee104CHn3Plm9k9y1OTN7CHgC+fclaGm/p/MbHJozHqRYkdJXiQYqTmTpJnFAw+Z2cn4YVdrAlWBP3Js8zPwcmjdCc65WWZ2CnAMPmkCJOBrwLn5j5ndA2zAJ93TgPF7EqCZjQNOAj4BhprZo/gm/m8P4XN9DAwLJfIewDfOudTQJYKWZnZBaL1koCH+C05Oe7781AOmA5/lWP9VM2uIn3Ur/gDH7wacY2Z/D71PAuqgceqlmFKSFykaLgUqA+2ccxlmthyfoPZyzn0T+hLQExhtZv8BtgCfOecuzscxbnPOvbPnTahG/BfOuUWhWv6ZwMOhGndeLQM5t00zs6/w0472Ad7YczjgRufcpIPsItU51zrUevABcAMwDD8u+pfOud6hmxS/OsD2BpzvnFuYn3hFop2uyYsUDcnA+lCC7wzU3X8FM6sbWucFfDN2W+AH4AQz23ONvaSZNcrnMb8BeoW2KQX0Br41sxrALufcGGBo6Dj7ywi1KORmLP4ywEn4CU0IPV+/ZxszaxQ6Zq6cc9uAm4C/h7ZJBlaHFvfPseoOoEyO95OAGy3UrGFmbQ50DJHiQElepGh4HWhvZtPwtfpfc1nnVGCWmc0Ezgeecs5twCe9N8xsDj7pN8nPAZ1zM4BR+Pm+f8TPzDYTaIG/lj0LuBt4IJfNRwJz9tx4t59PgZOByc659FDZi8B8YIaZzQOe5yAtiaFYZuOnNX0M36owBT8L2h5fAsfsufEOX+OPD8U2L/RepNhSFzoREZEopZq8iIhIlFKSFxERiVJK8iIiIlFKSV5ERCRKKcmLiIhEKSV5ERGRKKUkLyIiEqWU5EVERKLU/wOv7BIK00G44AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Specify pos_label as 2 (treating class 2 as the positive class)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, ada_clf.decision_function(X_test), pos_label=2)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
